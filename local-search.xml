<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>学习笔记|大模型微调方法</title>
    <link href="/blog/2024/01/18/2024/01/18/weitiao/"/>
    <url>/blog/2024/01/18/2024/01/18/weitiao/</url>
    
    <content type="html"><![CDATA[<h1 id="学习笔记-大模型微调方法"><a href="#学习笔记-大模型微调方法" class="headerlink" title="学习笔记|大模型微调方法"></a>学习笔记|大模型微调方法</h1><p>最近在看一些模型微调的相关方法，阅读了不少论文，想做个小结，分享给需要的朋友。</p><h2 id="直接微调"><a href="#直接微调" class="headerlink" title="直接微调"></a>直接微调</h2><p>直接微调是指在不改变模型结构的基础上 ，加载预训练模型并在有限的数据集上微调训练，通常选取更小的学习率，通常的一些技术有全量微调，冻结部分参数等。</p><ul><li>全量微调</li></ul><p>全量微调是指每次更新模型的全部参数，这种方式在已有预训练权重的基础上进行，训练和预训练一样，但是由于预训练模型已经学习到了一定的先验知识，所以会降低微调的训练成本，模型可以更快地收敛。</p><ul><li>部分微调（冻结）</li></ul><p>部分微调是指冻结模型的一部分层的参数不参与梯度更新的过程，这种方式在某些情况下被证明具有比全量微调更大的优势。除了简单的冻结层之外，不同的论文提出了不同的冻结策略，比如只微调偏置bias，或者只对归一化层等进行调整。</p><h2 id="适应性微调（Adapter）"><a href="#适应性微调（Adapter）" class="headerlink" title="适应性微调（Adapter）"></a>适应性微调（Adapter）</h2><p>Adapter最初由论文“Parameter-Efficient Transfer Learning for NLP”提出，策略是在Transformer的模型结构中在注意力层和前馈网络层之后加入两层全连接层和非线性激活函数，被称之为Adapter，第一层将d维向量映射到m维向量，第二层将m维向量映射回d维，以保持后续正确的残差连接，一个Adapter的总参数量为$(dm+m)+(md+d)$.如果你了解Transformer的实现，那么并不难知道Adapter加在哪里。</p><figure>    <img src=微调1.jpg" alt="DGL" style="width:90%;">    <figcaption>图1: LoRA</figcaption></figure><p>之后，借鉴Adapter的方法，论文“LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS”提出了LoRA低秩自适应微调技术。LoRA的策略是在Transformer的自主力层中，对q、v（注意力层中计算的三个向量分别是q、k、v）加入额外的类似于漏斗的全连接网络，与Adapter相似，第一个全连接层将维度缩小到r，第二个全连接层将维度恢复，之后将计算的q值与LORA相加。更精炼的概括是，LoRA在自注意力子层之间的q和v结果中添加了两个低秩矩阵进行微调。</p><figure>    <img src="微调2.jpg" alt="DGL" style="width:90%;">    <figcaption>图2: Adapter</figcaption></figure><h2 id="提示微调（P-Tuning）"><a href="#提示微调（P-Tuning）" class="headerlink" title="提示微调（P-Tuning）"></a>提示微调（P-Tuning）</h2><p>与P-Tuning相关的工作：</p><ul><li><p>论文 “The Power of Scale for Parameter-Efficient Prompt Tuning”，Prompt Tuning的首创。</p></li><li><p>论文 ”Li and Liang, “Prefix-Tuning: Optimizing Continuous Prompts for Generation”，提出前缀调整Prefix-Tuning。</p></li><li><p>论文“P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks”在二者之上改进提出了P-Tuning v2。</p></li></ul><p>当然工作并不止这些，相关内容参考<a href="https://zhuanlan.zhihu.com/p/464876160">知乎专栏</a></p><p>以上是目前主流的三大类微调方法，每种类别下又有这各式各样不同的小设计，尽管很多方法最先出自NLP领域，但随着技术的不断发展，CV与NLP领域的交叉融合趋势不断演进，典型代表是ViT模型。一起期待未来更多高效的微调方法的出现。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>fine_tuning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>学习笔记|Huggingface Accelerate</title>
    <link href="/blog/2024/01/13/2024/01/13/accelerate/"/>
    <url>/blog/2024/01/13/2024/01/13/accelerate/</url>
    
    <content type="html"><![CDATA[<h1 id="Huggingface-Accelerate学习笔记"><a href="#Huggingface-Accelerate学习笔记" class="headerlink" title="Huggingface|Accelerate学习笔记"></a>Huggingface|Accelerate学习笔记</h1><p>PyTorch 很灵活。 它允许您根据需要自定义它。 这意味着您还必须处理所有低级硬件自定义，而在 95% 的项目中您实际上并不关心这些。<br>PyTorch 的主要痛点之一是使代码适应各种硬件配置（CPU&#x2F;GPU&#x2F;TPU）。 您必须维护大量用于混合精度训练、梯度累积等的模版代码。尽管一些高级库完全抽象了所有工程组件（包括训练循环），但您仍然需要熟悉它们的 API 。 您还必须学习特定的方法和函数来覆盖它们以注入您的自定义行为。<br>但是，如果有一个库仅抽象出多 GPU&#x2F;TPU&#x2F;fp16 所需的样板代码并允许您完全按原样使用原始 PyTorch 代码呢？ HuggingFace Accelerate 就是专门为此目的而创建的！<br>在本文中，我们将了解 HuggingFace Accelerate 所提供的功能以及执行分布式训练&#x2F;评估以及权重和偏差集成是多么简单。<br>这是我们将要介绍的内容：</p><ul><li><p>你为什么要使用Huggingface Accelerate？</p></li><li><p>安装并配置Huggingface Accelerate</p></li><li><p>对比：典型pytorch训练循环</p></li><li><p>对比：Huggingface Accelerate训练循环</p></li><li><p>Accelerate类</p></li><li><p>梯度累积</p></li><li><p>梯度裁剪</p></li><li><p>分布式评估</p></li><li><p>执行过程</p></li><li><p>打印</p></li><li><p>延迟执行</p></li><li><p>保存加载模型</p></li><li><p>日志</p></li><li><p>运行分布式代码</p></li><li><p>其他功能</p></li></ul><h2 id="为什么应该使用-HuggingFace-Accelerate？"><a href="#为什么应该使用-HuggingFace-Accelerate？" class="headerlink" title="为什么应该使用 HuggingFace Accelerate？"></a>为什么应该使用 HuggingFace Accelerate？</h2><p>在继续阅读本文之前，您可能会对为什么应该首先使用 Accelerate 有疑问。 它实际上解决了什么问题？<br>Accelerate 解决的主要问题是分布式训练。 例如，在项目开始时，您可能会在单个 GPU 上运行模型来测试某些内容，但随着项目的发展加速您的训练，您可能会觉得需要将现有代码扩展到多 GPU 系统 。<br>在这种情况下，您实际上可以使用完全相同的代码通过 HuggingFace Accelerate 在 CPU&#x2F;GPU&#x2F;多 GPU&#x2F;TPU 上进行训练，而这对于纯 PyTorch 来说是不可能的。 在那里，您必须编写一堆 if-else 语句，以使您的管道足够强大，可以在任何类型的训练设置上运行。 如果您想调试 PyTorch 代码，那么在 CPU 上运行代码通常会很有帮助，因为它会产生更有意义的错误，而不是在 GPU 上。<br>啊，但是等等，还有更多。 以下列出了使用 Accelerate 的其他一些优势：</p><ul><li><p>您可以删除处理不同训练设置（CPU&#x2F;GPU&#x2F;TPU）所需的样板代码。</p></li><li><p>您还可以使用相同的代码在 CPU、GPU、多 GPU 和多节点上进行训练。</p></li><li><p>这是进行分布式评估的便捷方法。</p></li><li><p>允许您删除混合精度和梯度累积所需的样板。</p></li><li><p>增强分布式系统中的日志记录和跟踪。</p></li><li><p>可以方便地保存分布式系统中的训练状态。</p></li><li><p>完全分片的数据并行训练。</p></li><li><p>集成DeepSpeed。</p></li><li><p>集成各种实验跟踪器（ex: Weights &amp; Biases），以便方便地记录分布式系统。</p></li><li><p>附带一个方便的 CLI 命令，用于启动分布式训练。</p></li><li><p>在 Jupyter Notebook 中启动分布式训练的便捷功能。</p></li></ul><p>我们将在本文中一一研究这些功能。</p><h2 id="安装和配置-HuggingFace-Accelerate"><a href="#安装和配置-HuggingFace-Accelerate" class="headerlink" title="安装和配置 HuggingFace Accelerate"></a>安装和配置 HuggingFace Accelerate</h2><p>当然，在使用 HuggingFace Accelerate 之前，您必须安装它。 您可以通过 pip 或 conda 来完成：</p><div class="code-wrapper"><pre><code class="hljs bash">pip install accelerateORconda install -c conda-forge accelerate</code></pre></div><p>Accelerate 是一个快速更新的库，每天都会添加新功能。 我更喜欢从 GitHub 存储库安装它以使用尚未发布的功能。 您可以通过在终端中运行以下命令来执行此操作：</p><div class="code-wrapper"><pre><code class="hljs bash">pip install git+https://github.com/huggingface/accelerate</code></pre></div><p>安装 Accelerate 后，您应该为当前系统配置它。 为此，请运行以下命令并回答提示的问题：</p><div class="code-wrapper"><pre><code class="hljs bash">accelerate config</code></pre></div><p>如果您不想配置这些文件，<code>accelerate config default</code>可以快速完成对accelerate的配置。</p><p>完成后，要检查您的配置是否正常，您可以运行：</p><div class="code-wrapper"><pre><code class="hljs bash">accelerate <span class="hljs-built_in">env</span></code></pre></div><p>下面是一个示例输出，它描述了一台机器上使用混合精度的两个 GPU。</p><div class="code-wrapper"><pre><code class="hljs bash">- `Accelerate` version: 0.11.0.dev0- Platform: Linux-5.10.0-15-cloud-amd64-x86_64-with-debian-11.3- Python version: 3.7.12- Numpy version: 1.19.5- PyTorch version (GPU?): 1.12.0+cu102 (True)- `Accelerate` default config:        - compute_environment: LOCAL_MACHINE        - distributed_type: MULTI_GPU        - mixed_precision: fp16        - use_cpu: False        - num_processes: 2        - machine_rank: 0        - num_machines: 1        - main_process_ip: None        - main_process_port: None        - main_training_function: main        - deepspeed_config: &#123;&#125;        - fsdp_config: &#123;&#125;</code></pre></div><h2 id="比较与对比：典型的-PyTorch-训练循环"><a href="#比较与对比：典型的-PyTorch-训练循环" class="headerlink" title="比较与对比：典型的 PyTorch 训练循环"></a>比较与对比：典型的 PyTorch 训练循环</h2><p>以下是您必须熟悉的基本 PyTorch 训练循环：</p><div class="code-wrapper"><pre><code class="hljs python">device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> training_dataloader:    optimizer.zero_grad()    inputs, targets = batch    inputs = inputs.to(device)    targets = targets.to(device)    outputs = model(inputs)    loss = loss_function(outputs, targets)    optimizer.step()    scheduler.step()</code></pre></div><p>这是一个基本的训练循环，只能在 CPU 或单个 GPU 上运行（而且，它不支持混合精度和梯度累积等现代技术）。<br>为了实现分布式训练和fp16&#x2F;梯度累积，你需要添加一堆if-else语句，这使得代码难以维护并且容易出错。<br>接下来，您将看到🤗 Accelerate 如何让您无缝集成多 GPU&#x2F;TPU&#x2F;多节点训练，同时只需几行额外代码即可支持混合精度和梯度累积。</p><h2 id="比较和对比：HuggingFace-加速训练循环"><a href="#比较和对比：HuggingFace-加速训练循环" class="headerlink" title="比较和对比：HuggingFace 加速训练循环"></a>比较和对比：HuggingFace 加速训练循环</h2><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Acceleratoraccelerator = Accelerator()model, optimizer, training_dataloader, scheduler = accelerator.prepare(     model, optimizer, training_dataloader, scheduler)<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> training_dataloader:    optimizer.zero_grad()    inputs, targets = batch    outputs = model(inputs)    loss = loss_function(outputs, targets)    accelerator.backward(loss)    optimizer.step()    scheduler.step()</code></pre></div><p>上面的训练循环能够在CPU&#x2F;GPU&#x2F;多GPU&#x2F;TPU&#x2F;多节点上运行。 请注意，只需很少的改变即可将现有的原始 PyTorch 代码转换为更强大的形式，可以轻松扩展到您想要的任何硬件！<br>让我们更详细地看看上面的训练循环。<br>首先，我们导入 Accelerator 主类并实例化它：</p><div class="code-wrapper"><pre><code class="hljs pyth">from accelerate import Acceleratoraccelerator = Accelerator()</code></pre></div><blockquote><p>注意：Accelerator 类应该在脚本开始时实例化，或者尽早实例化，以便在整个脚本中使用 Accelerator 提供的方便的函数&#x2F;方法。</p></blockquote><p>您应该删除所有现有的 .cuda() 或 .to(device) 调用。 加速器对象将自动为您处理此问题，并将这些对象放置在正确的设备上。<br>如果您出于某种原因想要停用自动设备放置并且想要自己执行此操作，则可以通过在初始化 Accelerator 类时传递 device_placement&#x3D;False 来停用它。<br>接下来，您需要将模型、优化器、训练&#x2F;验证数据加载器和学习率调度程序传递给 Accelerator.prepare() 方法。 对象提供给prepare() 方法的顺序并不重要——重要的是它们按照传递的顺序解包。 这将为训练做好一切准备。<br>例如，HuggingFace Accelerate 会将数据加载器分片到所有可用的 GPU&#x2F;TPU 核心上，以便每个核心看到训练数据集的不同部分。 此外，所有进程的随机状态将在每次迭代开始时同步。</p><blockquote><p>实际批量大小将是所使用的设备数量乘以您在脚本中设置的批量大小。 例如：在创建训练数据加载器时使用 4 个 GPU 进行训练，批量大小为 16，这将有效地在实际批量大小为 16*4 &#x3D; 64 的情况下进行训练。</p></blockquote><blockquote><p>仅当调度器（scheduler）需要在每个优化器步骤中逐步执行时，才应将学习率调度程序传递给prepare()。</p></blockquote><p>此外，如果您正在分布式设置上进行训练，您的训练数据加载器长度可能会发生变化。 因此，任何使用训练数据加载器长度的指令（例如：如果您想记录训练步骤的总数）都应该在prepare()方法之后调用。<br>您可能会注意到，Accelerator 是将完整的 HuggingFace Accelerate 框架绑定在一起的主类。 事实上，让我们详细看看。</p><h2 id="Accelerate类"><a href="#Accelerate类" class="headerlink" title="Accelerate类"></a>Accelerate类</h2><p>Accelerator 类是这个完整框架的关键，它还有一些有用的方法，您将在本文的后续部分中看到。 Accelerate 类在实例化时采用的最重要的参数如下所述：</p><ul><li><p><code>device_placement</code>：如果您希望 Accelerate 自动将对象放置在适当的设备上，请设置为 True。 理想情况下，应该打开此功能。 您可以将其关闭以手动放置对象。</p></li><li><p><code>split_batches</code>：如果设置为 True，则批次将跨设备拆分。 例如：如果您在启动数据加载器时在 4-GPU 机器上进行训练，批量大小为1有4个数据的数据集，则每个 GPU 上的实际批量大小将为 4&#x2F;4 &#x3D; 1。如果设置为 False，则有效 所有 GPU 的批量大小将为 4*4 &#x3D; 16。理想情况下，应将其设置为 False。</p></li><li><p><code>mix_ precision</code>： Accelerate 自动处理混合精度逻辑，您无需编写 if-else 语句即可在混合精度和全精度之间切换。 传递“no”以禁用混合精度。 要启用混合精度，只需传递“fp16”即可。 Accelerate 还支持 bf16（通过“bf16”来启用它）。</p></li><li><p><code>gradient_accumulation_steps</code>：🤗 Accelerate 还会自动处理梯度累积逻辑，减少大量样板代码。 只需传递梯度累积步骤的数量，Accelerate 将通过上下文管理器（with accelerator.accumulate）完成其余的工作，正如您将在本文后面看到的那样。</p></li><li><p><code>cpu</code>：如果为 True，则即使 GPU 可用，也会强制在 CPU 上进行训练。 对于调试目的很有用。</p></li><li><p><code>log_with</code>：用于记录的实验跟踪器。 要使用W&amp;B ，请传递 wandb，您就可以使用 W&amp;B 进行实验跟踪。</p></li><li><p><code>Accelerate</code> 的方法还有很多，这里不可能一一涵盖。 官方文档非常好，您可以查看参数。</p></li></ul><h2 id="进行梯度累积"><a href="#进行梯度累积" class="headerlink" title="进行梯度累积"></a>进行梯度累积</h2><p>如果您需要训练更大的批量大小但 GPU 内存有限，梯度累积是一个不错的策略。<br>梯度累积通过累积指定步数的梯度来模拟更大的批量大小。 要在 HuggingFace Accelerate 中使用梯度累积，您只需在启动 Accelerator 类时将gradient_accumulation_steps 传递到所需的数字，并将训练循环包装在accumulate() 上下文管理器中。<br>这是启用梯度累积的 HuggingFace Accelerate 训练循环：</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Acceleratoraccelerator = Accelerator(gradient_accumulation_steps=<span class="hljs-number">2</span>)model, optimizer, training_dataloader, scheduler = accelerator.prepare(     model, optimizer, training_dataloader, scheduler)<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> training_dataloader:    <span class="hljs-comment"># accumulate context manager</span>    <span class="hljs-keyword">with</span> accelerate.accumulate(model):<span class="hljs-comment"># you can also use accelerator.accumulate</span>        optimizer.zero_grad()        inputs, targets = batch        outputs = model(inputs)        loss = loss_function(outputs, targets)        accelerator.backward(loss)        optimizer.step()        scheduler.step()</code></pre></div><h2 id="执行梯度裁剪"><a href="#执行梯度裁剪" class="headerlink" title="执行梯度裁剪"></a>执行梯度裁剪</h2><p>梯度裁剪是一种有用的技术，可以避免神经网络中梯度爆炸问题。 如果您要执行混合精度的梯度裁剪，则应首先取消缩放梯度。<br>以下是 PyTorch 文档中的声明：</p><blockquote><p>所有由scaler.scale(loss).backward()产生的梯度都会被缩放。 如果你想修改或检查backward()和scaler.step(optimizer)之间参数的.grad属性，你应该首先取消缩放它们。 例如，梯度裁剪操作一组梯度，使其全局范数（参见 torch.nn.utils.clip_grad_norm_()）或最大幅度（参见 torch.nn.utils.clip_grad_value_()）&lt;&#x3D;某个用户施加的阈值 。 如果您尝试在不取消缩放的情况下进行clip，则梯度的范数&#x2F;最大幅度也会缩放，因此您请求的阈值（这意味着未缩放梯度的阈值）将无效。</p></blockquote><blockquote><p>unscale_ 只能在每个优化器的每个step调用中调用一次，并且只能在该优化器分配的参数的所有梯度都已累积之后调用。 在每个step之间对给定优化器调用 unscale_ 两次会触发运行时错误。</p></blockquote><p>纯 PyTorch 中混合精度和梯度裁剪的训练循环如下所示：</p><div class="code-wrapper"><pre><code class="hljs python">scaler = GradScaler()<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> epochs:    <span class="hljs-keyword">for</span> <span class="hljs-built_in">input</span>, target <span class="hljs-keyword">in</span> data:        optimizer.zero_grad()        <span class="hljs-keyword">with</span> autocast():            output = model(<span class="hljs-built_in">input</span>)            loss = loss_fn(output, target)        scaler.scale(loss).backward()        <span class="hljs-comment"># Unscales the gradients of optimizer&#x27;s assigned params in-place</span>        scaler.unscale_(optimizer)        <span class="hljs-comment"># Since the gradients of optimizer&#x27;s assigned params are unscaled, clips as usual:</span>        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)        <span class="hljs-comment"># optimizer&#x27;s gradients are already unscaled, so scaler.step does not unscale them,</span>        <span class="hljs-comment"># although it still skips optimizer.step() if the gradients contain infs or NaNs.</span>        scaler.step(optimizer)        <span class="hljs-comment"># Updates the scale for next iteration.</span>        scaler.update()</code></pre></div><p>正如您所看到的，在我们的训练循环中同时使用混合精度、梯度裁剪和梯度累积可能会导致大量的样板代码。 结果，代码维护变得更加困难。<br>幸运的是，使用 HuggingFace Accelerate 可以更有效地完成梯度裁剪：</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Acceleratormax_grad_norm = <span class="hljs-number">1.0</span>accelerator = Accelerator(mixed_precision=<span class="hljs-string">&quot;fp16&quot;</span>, gradient_accumulation_steps=<span class="hljs-number">2</span>)model, optimizer, training_dataloader, scheduler = accelerator.prepare(     model, optimizer, training_dataloader, scheduler)<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> training_dataloader:    <span class="hljs-comment"># accumulate context manager (for gradient accumulation)</span>    <span class="hljs-keyword">with</span> accelerate.accumulate(model):        optimizer.zero_grad()        inputs, targets = batch        outputs = model(inputs)        loss = loss_function(outputs, targets)        accelerator.backward(loss)<span class="hljs-comment"># gradient clipping</span><span class="hljs-keyword">if</span> accelerator.sync_gradients:            accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)        optimizer.step()        scheduler.step()</code></pre></div><p>上面的训练循环执行混合精度训练、梯度累积和梯度裁剪。 请注意，与纯 PyTorch 训练循环相比，训练循环看起来多么干净。<br>让我们深入研究一下梯度裁剪部分发生了什么：<br><code>Accelerator.sync_gradients</code> 检查梯度当前是否在所有进程之间同步。 您应该使用 <code>Accelerator.clip_grad_norm_ </code>而不是 <code>torch.nn.utils.clip_grad_norm_ </code>。 在幕后，Accelerate 的 <code>Clip_grad_norm_ </code>在剪切梯度之前执行梯度缩放。 您可以查看源代码，并查看上述所讨论概念的问题。</p><h2 id="执行分布式评估"><a href="#执行分布式评估" class="headerlink" title="执行分布式评估"></a>执行分布式评估</h2><p>如果您以前曾经尝试过使用纯 PyTorch 执行分布式评估，那么您就会知道它有多么具有挑战性。 HuggingFace Accelerate 提供了一种尽可能轻松地执行分布式评估的便捷方法。<br>您可以使用 HuggingFace Accelerate 的 <code>Gather_for_metrics()</code> 方法从所有进程中收集所有预测和标签以计算指标。 此外，<code>gather_for_metrics()</code> 会删除最后一批中的重复项，因为数据集末尾的某些数据可能会重复，以便批次可以在所有workers之间平均分配。 Gather_for_metrics() 会在收集时自动删除重复的数据，以便您的指标计算正确。 这是演示分布式评估的简短代码片段。</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">for</span> inputs, targets <span class="hljs-keyword">in</span> validation_dataloader:    predictions = model(inputs)    <span class="hljs-comment"># Gather all predictions and targets</span>    all_predictions, all_targets = accelerator.gather_for_metrics((predictions, targets))    metrics = calculate_metrics(all_predictions, all_targets)</code></pre></div><p>如果您不想执行分布式评估而只想执行分布式训练，那么您可以将验证数据加载器保留在prepare()方法之外。</p><h2 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h2><p>HuggingFace Accelerate 还提供了一些方便的方法来在分布式系统上执行您可能喜欢的流程。<br>以下大部分内容取自此处的 Accelerate 文档。</p><h3 id="在每个server上执行"><a href="#在每个server上执行" class="headerlink" title="在每个server上执行"></a>在每个server上执行</h3><p>如果您使用多个server并且希望某些内容在每个server（节点）上执行一次，那么您可以使用 <code>is_local_main_process</code>。</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">if</span> accelerator.is_local_main_process:    do_thing_once_per_server()</code></pre></div><p>大多数加速器方法也有一个可供您使用的装饰器对应项。 例如，您可以使用 <code>on_local_main_process()</code> 装饰器包装函数，以在函数执行时实现相同的行为：</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-meta">@accelerator.on_local_main_process</span><span class="hljs-keyword">def</span> <span class="hljs-title function_">do_my_thing</span>():    <span class="hljs-string">&quot;Something done once per server&quot;</span>    do_thing_once_per_server()</code></pre></div><h3 id="仅在一个server执行"><a href="#仅在一个server执行" class="headerlink" title="仅在一个server执行"></a>仅在一个server执行</h3><p>对于只应在所有servers上执行一次的语句（即多节点情况下只有一个节点上的一个进程是主进程），请使用 <code>is_main_process</code>：</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">if</span> accelerator.is_main_process:    do_thing_once()</code></pre></div><p>同样，您可以使用装饰器对应部分来包装函数的执行：</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-meta">@accelerator.on_main_process</span><span class="hljs-keyword">def</span> <span class="hljs-title function_">do_my_thing</span>():    <span class="hljs-string">&quot;Something done once per server&quot;</span>    do_thing_once()</code></pre></div><p>关于具体流程<br>如果一个函数应该在特定的硬件或本地进程index上运行，则有类似的装饰器可以实现此目的：</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-meta">@accelerator.on_local_process(<span class="hljs-params">local_process_idx=<span class="hljs-number">0</span></span>)</span><span class="hljs-keyword">def</span> <span class="hljs-title function_">do_my_thing</span>():    <span class="hljs-string">&quot;Something done on process index 0 on each server&quot;</span>    do_thing_on_index_zero_on_each_server()</code></pre></div><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-meta">@accelerator.on_process(<span class="hljs-params">process_index=<span class="hljs-number">0</span></span>)</span><span class="hljs-keyword">def</span> <span class="hljs-title function_">do_my_thing</span>():    <span class="hljs-string">&quot;Something done on process index 0&quot;</span>    do_thing_on_index_zero()</code></pre></div><h2 id="打印"><a href="#打印" class="headerlink" title="打印"></a>打印</h2><p>在每个进程上打印并不是一个好主意，因为它会堵塞控制台日志并使它们无法读取。 为了每个进程只打印一次，HuggingFace Accelerate 有它的 print 方法，这是一种打印东西的便捷方法。 用 Accelerate 的打印方法替换常用的打印。 在底层，它只是检查进程是否是本地主进程。 </p><div class="code-wrapper"><pre><code class="hljs python">accelerator.<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;My thing I want to print!&quot;</span>)</code></pre></div><h2 id="等待（Deferring）执行"><a href="#等待（Deferring）执行" class="headerlink" title="等待（Deferring）执行"></a>等待（Deferring）执行</h2><p>有时您可能需要推迟（推迟）一些执行。 当您运行 Python 脚本时，指令会按顺序执行。 当您处于分布式设置（即在多个 GPU 上运行脚本）时，每个进程（或 GPU）将按顺序执行所有指令。 某些进程可能比其他进程更快地执行指令。<br>您可能需要等待所有进程到达某个点，然后再执行进一步的指令。 例如，在保存模型之前，您应该确保所有进程都已执行指令（即所有进程都应该完成训练）。 要等待所有进程到达脚本中的某个点，您可以在该特定点使用 Accelerate 的 <code>wait_for_everyone()</code> 。</p><div class="code-wrapper"><pre><code class="hljs python">accelerator.wait_for_everyone()</code></pre></div><p>该指令将阻塞所有首先到达的进程，直到所有其他进程都到达该点。</p><h2 id="保存和加载状态"><a href="#保存和加载状态" class="headerlink" title="保存和加载状态"></a>保存和加载状态</h2><p>如果您想在脚本开始时保存在prepare()方法中传递的任何对象&#x2F;模型，您应该使用<code>unwrap_model()</code>来删除在分布式过程中添加的所有特殊模型包装器。 您应该使用 Accelerate 的 <code>save()</code> 而不是 <code>torch.save()</code>。 在幕后，Accelerate 的 <code>save()</code> 方法为每台机器或服务器保存一次对象。 您可以查看源代码。 此外，在使用 <code>wait_for_everyone()</code> 保存模型之前，暂停已完成的进程直到所有进程完成（如上一节所述）也很有用。 这是一个严格遵循上述要点的简短示例。</p><div class="code-wrapper"><pre><code class="hljs python">model = MyModel()model = accelerator.prepare(model)accelerator.wait_for_everyone()<span class="hljs-comment"># Unwrap</span>model = accelerator.unwrap_model(model)state_dict = model.state_dict()<span class="hljs-comment"># Use accelerator.save()</span>accelerator.save(state_dict, <span class="hljs-string">&quot;my_state.pkl&quot;</span>)</code></pre></div><p>您可能经常想保存并在之后继续训练状态。 这样做需要保存和加载模型、优化器、RNG 生成器和 GradScaler。 HuggingFace Accelerate 内部有两个方便的功能可以快速实现这一目标：</p><ul><li><p>使用 <code>save_state()</code> 将上述所有内容保存到文件夹位置</p></li><li><p>使用 <code>load_state()</code> 加载从早期 <code>save_state</code> 存储的所有内容</p></li></ul><p>您还可以通过 <code>register_for_checkpointing()</code> 方法注册自定义对象来保存它们。 只要对象具有 <code>state_dict</code> 和 <code>load_state_dict</code> 功能并且已注册检查点，HuggingFace Accelerate 就可以使用上述两种方法保存和加载任何对象。<br>这是一个在训练期间使用检查点保存和重新加载状态的示例（从 HuggingFace Accelerate 文档获取和修改）</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator<span class="hljs-keyword">import</span> torchaccelerator = Accelerator()my_scheduler = torch.optim.lr_scheduler.StepLR(my_optimizer, step_size=<span class="hljs-number">1</span>, gamma=<span class="hljs-number">0.99</span>)my_model, my_optimizer, my_training_dataloader = accelerate.prepare(my_model, my_optimizer, my_training_dataloader)<span class="hljs-comment"># Register the LR scheduler</span>accelerate.register_for_checkpointing(my_scheduler)<span class="hljs-comment"># Save the starting state</span>accelerate.save_state(<span class="hljs-string">&quot;my/save/path&quot;</span>)<span class="hljs-comment"># Perform training</span><span class="hljs-comment"># training loop here ...</span><span class="hljs-comment"># Restore previous state</span>accelerate.load_state(<span class="hljs-string">&quot;my/save/path&quot;</span>)</code></pre></div><h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><p>HuggingFace Accelerate 有自己的日志记录实用程序来处理分布式系统中的日志记录。 您应该使用 Accelerate 的日志记录实用程序替换标准 Python 日志记录模块。 这是一个简短的例子：</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> accelerate.logging <span class="hljs-keyword">import</span> get_loggerlogger = get_logger(__name__)<span class="hljs-comment"># logs on all processes</span>logger.info(<span class="hljs-string">&quot;My log&quot;</span>, main_process_only=<span class="hljs-literal">False</span>)<span class="hljs-comment"># logs only on main process</span>logger.debug(<span class="hljs-string">&quot;My log&quot;</span>, main_process_only=<span class="hljs-literal">True</span>)</code></pre></div><h2 id="使用Weights-Biases进行实验跟踪"><a href="#使用Weights-Biases进行实验跟踪" class="headerlink" title="使用Weights &amp; Biases进行实验跟踪"></a>使用Weights &amp; Biases进行实验跟踪</h2><p>在分布式设置中使用实验跟踪器可能有点复杂，但 HuggingFace Accelerate 使我们变得相当容易。 要将Weights &amp; Biases与 HuggingFace Accelerate 结合使用，您应该在启动 Accelerator 类时首先将 wandb 传递给 <code>log_with</code> 参数。</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Acceleratoraccelerator = Accelerator(log_with=<span class="hljs-string">&quot;wandb&quot;</span>)</code></pre></div><p>在实验开始时，应使用 <code>Accelerator.init_trackers()</code> 来设置您的项目。 <code>init_trackers()</code> 接受以下参数。</p><ul><li><p>项目名称：项目的名称。 这将被传递到后台的 wandb.init() 项目参数中。</p></li><li><p>config：要记录的配置。 传递到 wandb.init() 的配置参数中。</p></li><li><p>init_kwargs：要传递给特定跟踪器的 <code>__init__</code> 函数的嵌套 kwargs 字典。 您可以传递 <code>wandb.init()</code> 在此参数中作为键值对的任何其他参数。</p></li></ul><p>以下是如何使用 HuggingFace Accelerate 初始化 W&amp;B 运行的示例。</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Acceleratoraccelerator = Accelerator()hps = &#123;<span class="hljs-string">&quot;num_epochs&quot;</span>: <span class="hljs-number">5</span>, <span class="hljs-string">&quot;learning_rate&quot;</span>: <span class="hljs-number">1e-4</span>, <span class="hljs-string">&quot;batch_size&quot;</span>: <span class="hljs-number">16</span>&#125;accelerator.init_trackers(    <span class="hljs-string">&quot;my_project&quot;</span>,    config=hps,    init_kwargs=&#123;        <span class="hljs-string">&quot;wandb&quot;</span>: &#123;            <span class="hljs-string">&quot;notes&quot;</span>: <span class="hljs-string">&quot;testing accelerate pipeline&quot;</span>,            <span class="hljs-string">&quot;tags&quot;</span>: [<span class="hljs-string">&quot;tag_a&quot;</span>, <span class="hljs-string">&quot;tag_b&quot;</span>],            <span class="hljs-string">&quot;entity&quot;</span>: <span class="hljs-string">&quot;gladiator&quot;</span>,        &#125;    &#125;,)</code></pre></div><p>初始化 W&amp;B 跟踪后，您现在可以使用 Accelerate 的 <code>log()</code> 方法记录任何数据，就像之前使用 <code>wandb.log()</code> 所做的那样。 您还可以传递当前step编号，将记录的数据与训练循环中的特定步骤相关联。</p><div class="code-wrapper"><pre><code class="hljs python">accelerator.log(&#123;<span class="hljs-string">&quot;train_loss&quot;</span>: <span class="hljs-number">1.12</span>, <span class="hljs-string">&quot;valid_loss&quot;</span>: <span class="hljs-number">0.8</span>&#125;, step=<span class="hljs-number">1</span>)</code></pre></div><p>完成训练后，请确保运行 <code>Accelerator.end_training()</code>，以便所有跟踪器都可以运行其完成功能（如果有）。 这类似于调用 <code>wandb.finish()</code> 来完成运行并上传所有数据。</p><div class="code-wrapper"><pre><code class="hljs python">accelerator.end_training()</code></pre></div><p>如果您想了解 HuggingFace Accelerate 在幕后的作用，可以查看WandBTracker 类。</p><h2 id="启动分布式代码"><a href="#启动分布式代码" class="headerlink" title="启动分布式代码"></a>启动分布式代码</h2><p>现在您已经了解了如何使用 HuggingFace Accelerate 来训练分布式设置，现在是时候启动我们已调整用于训练分布式设置的实际代码了。 第一步是将所有代码包装到 main() 函数中。</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator<span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():    accelerator = Accelerator()    model, optimizer, training_dataloader, scheduler = accelerator.prepare(        model, optimizer, training_dataloader, scheduler    )    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> training_dataloader:        optimizer.zero_grad()        inputs, targets = batch        outputs = model(inputs)        loss = loss_function(outputs, targets)        accelerator.backward(loss)        optimizer.step()        scheduler.step()<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:    main()</code></pre></div><p>您也可以将其他中间函数包装在主函数中：</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():   function_which_does_data_processing()   function_which_does_training()   function_which_does_evaluation()<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:    main()</code></pre></div><p>接下来，您需要通过 <code>accelerate launch</code> 来启动它。</p><blockquote><p>建议您在使用加速启动之前运行accelerate config来根据您的喜好配置环境。 否则，🤗 Accelerate 将根据您的系统设置使用非常基本的默认值。</p></blockquote><p>HuggingFace Accelerate 有一个特殊的 CLI 命令，可帮助您通过加速启动在系统中启动代码。 该命令包含在各种平台上启动脚本所需的所有不同命令，而您不必记住每个命令是什么。<br>您可以使用以下命令快速启动脚本：</p><div class="code-wrapper"><pre><code class="hljs bash">accelerate launch &#123;script_name.py&#125; --arg1 --arg2 ...</code></pre></div><p>只需将 <code>accelerate launch</code> 放在命令的开头，然后像平常一样将其他参数传递给脚本即可！</p><p>由于这会运行各种torch生成方法，因此也可以在此处修改所有预期的环境变量。 例如，以下是如何使用单个 GPU 加速启动：</p><div class="code-wrapper"><pre><code class="hljs python">CUDA_VISIBLE_DEVICES=<span class="hljs-string">&quot;0&quot;</span> accelerate launch &#123;script_name.py&#125; --arg1 --arg2 ...</code></pre></div><p>如果你想了解更多，请<a href="https://wandb.ai/gladiator/HF%20Accelerate%20+%20W&B/reports/Hugging-Face-Accelerate-Super-Charged-With-Weights-Biases--VmlldzoyNzk3MDUx">阅读原文</a></p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>huggingface</tag>
      
      <tag>accelerate</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>”今年“（2023）|“今年”（2024）</title>
    <link href="/blog/2023/12/31/2023/12/31/last/"/>
    <url>/blog/2023/12/31/2023/12/31/last/</url>
    
    <content type="html"><![CDATA[<h2 id="世界破破烂烂，小狗缝缝补补"><a href="#世界破破烂烂，小狗缝缝补补" class="headerlink" title="世界破破烂烂，小狗缝缝补补"></a><strong>世界破破烂烂，小狗缝缝补补</strong></h2><p>2023年社交媒体上走红了一句话：<strong>世界破破烂烂，小狗缝缝补补。</strong></p><p>我想这大概是很多人的真实写照，“一个民族，千百万人里面才出一个天才；人世间数百万个闲暇的小时流逝过去，方始出现一个真正的历史性时刻，人类星光璀璨的时辰“，这是《人类群星闪耀时》里的一句话，我们大多数人都是如此。每一个时代的人，都会跳着押韵的舞蹈去解决不同的问题，新世界的魅力在于充满无限的可能性，即使你不知道那是好是坏。看到一篇文章，前段时间，看到消息ofo创始人在美国再次创业，接连在纽约格拉梅西公园和曼哈顿办起多家连锁咖啡店。而他还依然欠着ofo用户的押金没有退，如今店门口标志性的红色霓虹灯已经寥寥可数。里面讲了一个冷笑话，“除了骗子口中的故事，没有什么工作能轻松还赚钱，对于年轻人们，找一份能缝补生活的副业，比月入十万更靠谱”。</p><figure>    <img src="图2.jpg" alt="CLIP" style="width:50%;"></figure><p>我想”今年“这一词可能是最后一次代表2023年了，甚至有点恍惚，大学的三年就这么过去了。00后是从疫情中走出的一代，我经历了高考延期，疫情封校，丰富的校园生活、零星的大学生活，似乎今年才有了真正的大学之感。渐渐走出象牙塔，仍然不能避免成长这个词，挫折、打击、怀疑、困顿，是兜兜转转还是在逆流而上。没有很棒的感觉，也不算太糟糕。我想我是做的不够好的，很多机会都没有把握住。</p><p>大学的这张白纸已经画满了，也许不是标准答案；不过现在，我想看看下一个三年。</p>]]></content>
    
    
    <categories>
      
      <category>沉思录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>thoughts</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023年度AI事件</title>
    <link href="/blog/2023/12/30/2023/12/30/AI%E5%A4%A7%E4%BA%8B%E4%BB%B6/"/>
    <url>/blog/2023/12/30/2023/12/30/AI%E5%A4%A7%E4%BA%8B%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="年度AI事件"><a href="#年度AI事件" class="headerlink" title="年度AI事件"></a>年度AI事件</h1><p>毫无疑问，2022年11月30日ChatGPT的发布，代表着人工智能新一轮的热潮，而2023年的这些故事将会塑造一个全新的未来：</p><h2 id="人工智能的进展"><a href="#人工智能的进展" class="headerlink" title="人工智能的进展"></a>人工智能的进展</h2><p>在今年的人工智能领域，可谓是神仙打架，国外的微软、谷歌、Meta等互相比拼，国内也爆发白模大战。这一年中，文生图以及文生视频等赛道都孵化出了令人惊艳的产品。</p><h3 id="图像生成"><a href="#图像生成" class="headerlink" title="图像生成"></a><strong>图像生成</strong></h3><ul><li><strong>Midjourney：</strong> <a href="https://journal.everypixel.com/top-ai-news-march-2023">Midjourney 的 V.5 模型</a>标志着图像生成领域的一个里程碑，展示了更高的效率、一致性和更高的分辨率。而最近最新的<a href="https://mid-journey.ai/midjourney-v6-release/">Midjourney V.6</a>带来了额外的增强功能，例如更准确的提示跟随、增加的模型知识和较小的文本绘制能力。</li></ul><figure>    <img src="1.jpg" alt="CLIP" style="width:50%;">    <figcaption>(上v6，下v5）</figcaption></figure>  <ul><li><strong>DALL·E 3：</strong><a href="https://journal.everypixel.com/top-ai-news-september-2023">DALL·E 3</a> 基于 ChatGPT 构建， 简化了图像生成，无需复杂的提示工程。此外，ChatGPT还引入了一项功能，可以帮助用户完善提示并根据反馈进行图像调整。</li></ul><figure>    <img src="2.jpg" alt="CLIP" style="width:80%;">    <figcaption>(左DELL·2，右DELL·3）</figcaption></figure><h3 id="视频生成"><a href="#视频生成" class="headerlink" title="视频生成"></a><strong>视频生成</strong></h3><ul><li><strong>Stability AI：</strong> Stability AI <a href="https://journal.everypixel.com/top-ai-news-november-2023">推出了 Stable Video Diffusion</a>，这是一种开创性的生成视频模型，在 GitHub 开源可访问。<a href="https://journal.everypixel.com/ai-image-statistics">与人工智能图像生成趋势</a>相似 ，稳定视频扩散模型很可能在大部分人工智能生成视频的创建中发挥关键作用。</li><li><strong>HeyGen：</strong> 人工智能初创公司推出了 <a href="https://the-decoder.com/heygen-offers-ai-powered-video-translation-with-impressive-lip-syncing-capabilities/">一款用于语音克隆</a>、嘴唇运动调整和视频语言翻译的工具。</li><li><strong>Runway Gen-2</strong>： <a href="https://research.runwayml.com/gen2">Runway 推出了 Gen-2</a> 模型，使用户能够轻松地从文本提示、图像或其他视频生成完整的视频。看看下面的例子。 </li><li><strong>Pika 和 Pika 1.0</strong>：随着最初的发布，Pika 获得了 50 万用户，每周生成数百万个视频。<a href="https://pika.art/launch">随后Pika 1.0</a>中升级的AI模型 使用户能够创建和编辑各种风格的视频，包括3D动画、动漫、卡通和电影。</li><li><strong>Meta 的编解码器头像：</strong> <a href="https://youtu.be/MVYrJJNdrEg?si=NR3DJMeYOfbiAunX">Meta 的</a> 用于视频中 3D 人脸的像素编解码器头像 (PiCA) 模型更逼真的呈现效果。</li></ul><h3 id="文本生成"><a href="#文本生成" class="headerlink" title="文本生成"></a><strong>文本生成</strong></h3><p><strong>文本生成</strong></p><ul><li><strong>Bard 和 Gemini：</strong> <a href="https://journal.everypixel.com/top-ai-news-march-2023">Google 的 Bard</a> 在聊天机器人领域添加了类人的情感和情绪。<a href="https://blog.google/technology/ai/google-gemini-ai/">谷歌的 Gemini</a>引入巴德聊天机器人并在多模态数据集上进行训练， 成为“最有能力”的人工智能模型，也是 OpenAI 的 ChatGPT 最接近的竞争对手。</li><li><strong>Grok：</strong> <a href="https://journal.everypixel.com/top-ai-news-april-2023">Elon Musk 的初创公司 xAI通过</a><a href="https://journal.everypixel.com/top-ai-news-november-2023">推出“Grok”</a> ——一个幽默、叛逆、通过 𝕏 平台提供实时知识的聊天机器人， 标志着对人工智能开发的承诺，并有可能与 OpenAI 竞争 。xAI 承诺 Grok<a href="https://x.ai/">旨在回答</a>其他人工智能系统拒绝的挑衅性问题。</li><li><strong>OverflowAI：</strong> <a href="https://journal.everypixel.com/top-ai-news-july-2023">Stack Overflow 的 OverflowAI</a> 增强了知识管理，支持在 Visual Studio Code 和 Slack 中通过 AI 搜索相关答案。</li><li><strong>Llama 2：</strong> <a href="https://journal.everypixel.com/top-ai-news-july-2023">Meta 发布了 Llama 2</a>，这是其下一代开源大型语言模型，展示了增强的效率。Meta 经过微调的 LLM 还针对对话用例进行了优化，并且在大多数基准测试中都优于其他开源模型。</li><li><strong>GPT-4：</strong> <a href="https://journal.everypixel.com/top-ai-news-march-2023">OpenAI 的 GPT-4</a> 现在可以处理图像输入、生成字幕、分类、在来回对话中收听和响应，并支持 <a href="https://journal.everypixel.com/top-ai-news-september-2023">实时网页浏览</a>。OpenAI 还扩展了对插件的支持，促成了一个充满开源竞争对手的环境。GPT-4 是 OpenAI 开发 AGI 之旅的下一步。</li><li><strong>Mistral 7B：</strong> <a href="https://mistral.ai/">Mistral AI</a> 今年<a href="https://www.nytimes.com/2023/12/10/technology/mistral-ai-funding.html">估值约20亿美元</a>， 发布了Mistral 7B，这是一个挑战GPT-4和Claude 2的大型语言模型。Mistral AI强调开放的技术方法，提供免费下载其模型。</li><li><strong>Mixtral 8x7B：</strong> <a href="https://mistral.ai/news/mixtral-of-experts/">Mistral AI 还推出了 Mixtral 8x7B</a>，这是一种具有开放权重的高质量稀疏混合专家模型 (SMoE)，具有 46.7B 总参数，开创了模型的开放性，增强了真实性并减少了偏差。</li><li><strong>Yi-34B llm：</strong> 今年 <a href="https://techcrunch.com/2023/11/05/valued-at-1b-kai-fu-lees-llm-startup-unveils-open-source-model/">估值为 10 亿美元，李开复的初创公司</a><a href="http://01.ai/">01.AI</a> 发布了 Yi-34B，这是一个开源的模型，其参数数量显着高于竞争模型，强调了其成本效益。尽管之后陷入抄袭风波，但不可否认零一万物的成功。</li></ul><h2 id="国内外事件时间线"><a href="#国内外事件时间线" class="headerlink" title="国内外事件时间线"></a>国内外事件时间线</h2><ul><li><p>2022年11月30日，chatGPT问世。</p></li><li><p>2023年2月1日，chatGPT plus版本上线，OpenAI开启订阅付费计划。</p></li><li><p>2023年2月7日，微软将chatGPT功能集成到New Bing，当日微软市值暴涨 800 亿美元。</p></li><li><p>2023年2月7日，谷歌Bard首秀Demo并翻车，股票一夜之间暴跌 7000 亿人民币。</p></li><li><p>2023年2月24日，Meta发布LLaMa 并开源，带动了开源社区和AI社区的高速成长和发展。</p></li><li><p>2023年3月1日，OpenAI推出ChatGPT API，供开发者集成。</p></li><li><p>2023年3月14日 OpenAI发布GPT-4,并在ChatGPT和Bing中支持。</p></li><li><p>2023年3月14日，斯坦福发布Alpaca，一个由LLaMA 7B微调的大模型。</p></li><li><p>2023年3月16日 微软推出copilot（AI智能助手），重塑未来的生产方式。</p></li><li><p>2023年3月16日，百度发布文心一言，此后进入国内的百模大战。</p></li><li><p>2023年3月17日，微软GPT-4 Office全家桶发布。</p></li><li><p>2023年3月21日，Midjourney v5版本画出100%逼真情侣。</p></li><li><p>2023年3月22日，Runway 重磅发布Gen-2，文生视频里程碑。</p></li><li><p>2023年3月24日，OpenAI 为解除 ChatGPT 无法联网的限制，启动第三方插件支持功能。</p></li><li><p>2023年3月29日，千名大佬发联名信，叫停GPT-5超强大模型。</p></li><li><p>2023年3月31日，意大利暂时禁止ChatGPT使用，于4月28日恢复ChatGPT服务。</p></li><li><p>2023年4月6日，Meta发布可以分割一切的Segment Anything。</p></li><li><p>2023年4月20日，Google Brain与DeepMind 合并成立 Google DeepMind。</p></li><li><p>2023年5月5日，微软BingChat全面开放。</p></li><li><p>2023年7月13日，马斯克官宣成立xAI。</p></li><li><p>2023年7月19日，Llama 2开源可商用，模型系列包含 70 亿、130 亿和 700 亿三种参数。</p></li><li><p>2023年 8月10日，斯坦福「虚拟小镇」开源，引爆智能体（AI Agent）研究。</p></li><li><p>2023年8月29日，OpenAI发布企业版ChatGPT：没有限制、更快、更强、更安全的GPT-4。</p></li><li><p>2023年9月21日，OpenAI推出DALL·E 3，并将原生集成至ChatGPT中。 </p></li><li><p>2023年10月17日，文心大模型4.0发布。</p></li><li><p>2023年10月20日，ChatGPT全球宕机，API崩溃。</p></li><li><p>2023年10月29日，GPT-4集成图像上传 + 插件 + 代码运行器 + 文件上传 + 图像生成。</p></li><li><p>2023年11月15日，奥特曼被OpenAI董事会开除系列事件。</p></li><li><p>2023年11月29日 文生视频产品Pika 1.0正式发布。</p></li><li><p>2023年12月6日，谷歌DeepMind发布Gemini系列模型。</p></li><li><p>2023年12月14日，谷歌官宣开放Gemini API，奥特曼宣布ChatGPT Plus恢复订阅。</p></li><li><p>2023年12月21日，MidJounery V6 发布。</p></li></ul><hr><span style="color: gray;">参考文章：  <p><a href="https://journal.everypixel.com/2023-the-year-of-ai">https://journal.everypixel.com/2023-the-year-of-ai</a><br><a href="https://mp.weixin.qq.com/s/kpb86Nap6sqct6HqrtJc0g">https://mp.weixin.qq.com/s/kpb86Nap6sqct6HqrtJc0g</a><br></span></p>]]></content>
    
    
    <categories>
      
      <category>沉思录</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>论文解读——CLIP|Learning Transferable Visual Models From Natural Language Supervision</title>
    <link href="/blog/2023/12/29/2023/12/29/CLIP/"/>
    <url>/blog/2023/12/29/2023/12/29/CLIP/</url>
    
    <content type="html"><![CDATA[<h1 id="论文解读——CLIP-Learning-Transferable-Visual-Models-From-Natural-Language-Supervision"><a href="#论文解读——CLIP-Learning-Transferable-Visual-Models-From-Natural-Language-Supervision" class="headerlink" title="论文解读——CLIP|Learning Transferable Visual Models From Natural Language Supervision"></a>论文解读——CLIP|Learning Transferable Visual Models From Natural Language Supervision</h1><blockquote><p>摘要：本篇论文是OpenAI在多模态领域的成果之一，CLIP（Contrastive LanguageImage Pre-training）连接了文本和图像，旨在将同一语义的文字和图片转化到同一个隐空间下。论文主要贡献：</p><p>1.论文使用在互联网上收集的4亿个图像文本对的数据集从头训练一个模型，这种预测标题（文本）与图像的对应的预训练更像是一种广泛的监督学习，高效且可扩展。</p><p>2.在预训练好的模型上，能够实现zero-shot学习应用于下游任务。例如，在 ImageNet 零样本上可以媲美标准 ResNet50 的准确性（分类任务），而无需使用其所训练的 128 万个训练样本中的任何一个。</p><p>3.CLIP推动了基于文字引导的文字生成图像技术在图像生成领域的快速发展，是后续DALLE、Imagen、stable diffusion的基石。</p></blockquote><h2 id="CLIP如何将图像与文本映射到同一潜空间？"><a href="#CLIP如何将图像与文本映射到同一潜空间？" class="headerlink" title="CLIP如何将图像与文本映射到同一潜空间？"></a>CLIP如何将图像与文本映射到同一潜空间？</h2><p>在最开始，作者模仿之前类似的工作，通过训练一个图像CNN和一个文本Transformer来预测图像标题，但是很难继续下去，并且Transformer模型的计算量已经是原始ResNet50的两倍之多。</p><p>随后作者在表示学习的启发下，考虑探索一个系统，能够预测文本与图像两个整体的对应关系而不是文本的确切单词与图像的对应。事实证明，这样的方式效率提升了4倍。</p><p>作者设计了一种训练方式：给定$N$个（img，text），CLIP被训练并预测在$N×N$种组合中哪一种实际发生。</p><p>为了达到这个目标，作者使用了一个图像编码器（image encoder）和一个文本编码器（text encoder），如下图。</p><figure>    <img src="CLIP1.jpg" alt="CLIP" style="width:80%;">    <figcaption>图1: （text,image) encoder</figcaption></figure><p>之后，将图像和文本转化为embedding并最大化$N$个真实配对的图像文本之间的余弦相似度，同时最小化$N^2-N$个不正确配对的图像文本之间的余弦相似度。优化目标是这些相似性分数的平均交叉熵损失。</p><p>下面的伪代码先将图像、文本转化为嵌入向量，之后计算出图像和文本的余弦相似度（logits），最后分别计算图像交叉熵和文本的交叉熵，取二者的平均作为损失。</p><figure>    <img src="CLIP2.jpg" alt="CLIP" style="width:80%;">    <figcaption>图2: 计算余弦相似度</figcaption></figure><p>值得一提的是，训练过程的唯一数据增强是对Resized的图像进行随机方形裁剪。对于Softmax中控制对数范围的温度参数$\tau$将其转化为了对数参数化的乘法标量避免其作为超参数。</p><h2 id="CLIP模型结构设计"><a href="#CLIP模型结构设计" class="headerlink" title="CLIP模型结构设计"></a>CLIP模型结构设计</h2><p>对于图像编码器，作者考虑了两种主流架构。第一种是使用ResNet50，将全局平均池化替换为注意力池化机制，注意力池化层只使用一个简单的Transformer样式的多头QKV（query、key、value）注意力，query是图像的全局平均池化表示。第二种是ViT（Vision Transformer），在此模型上作者只做了微小修改，在Transformer输入之前的combined patch和position embedding后加了一个额外的归一化层，以及不同的初始化模型权重的方法。对于文本编码器，作者使用12层宽度为512的8头注意力Transformer。CLIP的整体表现对文本编码器并不敏感。</p><h2 id="Using-CLIP"><a href="#Using-CLIP" class="headerlink" title="Using CLIP"></a>Using CLIP</h2><p>用预训练好的CLIP在下游数据集上测试其零样本迁移学习的能力。对于每一个数据集，使用每个类的名字构造出文本，过程如下。</p><figure>    <img src="CLIP3.jpg" alt="CLIP" style="width:80%;">    <figcaption>图3: 应用于下游数据集</figcaption></figure><p>在27个下游数据集上评估，CLIP的零样本分类能力与标准ResNet50的完全监督线性分类器（指最后分类任务使用logistic还是linear）对比，发现在16个数据集上CLIP的表现都更加优秀，其中包括ImageNet。</p><figure>    <img src="CLIP4.jpg" alt="CLIP" style="width:50%;">    <figcaption>图4</figcaption></figure><p>在STL10数据集上作者发现CLIP可以得到SOTA性能（99.3%），而在细粒度数据集上，其中Food101和Stanford Cars上的CLIP性能比使用逻辑回归的ResNet50超过20%，而在Flowers 102和FGVCAircraft上，zero-shot CLIP的表现则低于10%以上。</p><p>Zero-shot CLIP在两个测量视频动作识别的数据集上的表现明显优于ResNet 50。在Kinetics 700上，CLIP的表现比ResNet 50高出14.5%。Zero-shot CLIP在UCF 101上的表现也比ResNet 50高出7.7%。作者推测CLIP能够从文本中提取更多的动态语义而分类只是名词（单个单词）。</p><p>Zero-shot CLIP在面对专门的、复杂的或抽象的任务时表现欠佳。例如卫星图像分类（EuroSAT和RESISC 45）、淋巴结肿瘤检测（PatchCamelyon）、计算合成场景中的物体（CLEVRCounts），以及自动驾驶相关任务，例如德国交通标志识别（GTSRB）、识别与最近汽车的距离（KITTI Distance）。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>CLIP尽管能够与ResNet-50的baseline竞争，但这并不是意味着它达到了SOTA，所以后续仍然有大量的工作需要进行来改进CLIP的任务学习能力与迁移学习能力。</p><p>聊一聊对于文本生成图像的贡献。实际上，通过文本引导图像生成的概念并非首次出现在CLIP中。在CLIP之前，就已经有研究工作在探索这一领域，例如 GAN（生成对抗网络）相关的研究，2014 年的条件GAN（Conditional GAN）和后续的文本到图像的生成模型。CLIP，由OpenAI在2021年提出，并且它对于这个领域的贡献显著。</p><ul><li><p>强大的跨模态理解<br>CLIP通过在大规模数据集上预训练来学习图像和文本之间的关联。这使得它能够理解和捕捉到丰富的、多样化的视觉概念和语言描述之间的关系。</p></li><li><p>零样本学习能力<br>CLIP展示了出色的零样本学习能力，即在没有看过特定任务的训练样本的情况下，它仍能执行该任务。这意味着CLIP可以理解并响应以前未见过的文本描述，生成或识别与之相关的图像内容。</p></li><li><p>开辟新应用<br>CLIP的出现推动了一系列新的应用和研究，例如基于文本的图像搜索、内容创建和图像理解等。它为后续的文本到图像的生成模型（如DALL·E）铺平了道路，这些模型能够更直接地将文本描述转换为图像。</p></li><li><p>新的研究方向<br>CLIP的设计和成功推动了对跨模态（如文本和图像）学习和表示的进一步研究。它展示了通过对齐和联合训练两种不同类型的数据（图像和文本）来构建更为强大和通用的模型的潜力。</p></li><li><p>高效的标注效率<br>传统上，许多机器学习模型依赖大量标注的数据。CLIP通过学习直接从自然语言描述中获取监督信息，显示了减少对手工标注数据依赖的可能性。</p></li></ul><p>总而言之，虽然CLIP不是第一个文本引导图像生成的项目，但它在理解跨模态关系、零样本学习、以及推动文本到图像生成领域的发展方面做出了重要的贡献。</p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>zero-shot</tag>
      
      <tag>transfer learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文解读——DiffFit|Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning</title>
    <link href="/blog/2023/12/28/2023/12/28/DiffFIT/"/>
    <url>/blog/2023/12/28/2023/12/28/DiffFIT/</url>
    
    <content type="html"><![CDATA[<h1 id="论文解读——DiffFit-Unlocking-Transferability-of-Large-Diffusion-Models-via-Simple-Parameter-Efficient-Fine-Tuning"><a href="#论文解读——DiffFit-Unlocking-Transferability-of-Large-Diffusion-Models-via-Simple-Parameter-Efficient-Fine-Tuning" class="headerlink" title="论文解读——DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning"></a>论文解读——DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning</h1><blockquote><p>摘要：本文提出了一种高效的文本生成图像模型的微调方法，命名为DiffFIT，在8个下游数据集上的测试都优于现有的微调策略。本文的主要贡献：</p><p>1.提出了DiffFIT的微调方法，旨在已有预训练DiT模型参数的基础上，微调训练0.12%的参数，在下游数据集上取得了比以往微调方式高效的结果。</p><p>2.通过直观的理论分析以及设计的消融实验，表明为什么简单的参数高效微调可以快速适应新的分布。</p></blockquote><h2 id="参数微调方法"><a href="#参数微调方法" class="headerlink" title="参数微调方法"></a>参数微调方法</h2><h3 id="直接微调"><a href="#直接微调" class="headerlink" title="直接微调"></a>直接微调</h3><p>不改变模型结构的基础上 ，加载预训练模型并在有限的数据集上微调训练，通常选取更小的学习率，如全量微调，冻结等技术。</p><h3 id="适应性微调（Adaptive-Fine-Tuning）"><a href="#适应性微调（Adaptive-Fine-Tuning）" class="headerlink" title="适应性微调（Adaptive Fine-Tuning）"></a>适应性微调（Adaptive Fine-Tuning）</h3><p>通过增加一些小的结构，在预训练模型权重的基础上，微调小的结构上的参数， 而预训练参数全部冻结，通过这种方式调整模型的行为。如LoRA在自注意力子层之间的q和v结果中添加了两个低秩矩阵进行微调。</p><h3 id="提示微调（Prompt-Tuning）"><a href="#提示微调（Prompt-Tuning）" class="headerlink" title="提示微调（Prompt Tuning）"></a>提示微调（Prompt Tuning）</h3><p>P-Tuning在模型输入上调整输入序列的内容，加入一些可训练的提示或标志，以此来改善模型在有提示下的特定任务上的表现。</p><p>（后续或许会单独写一下这些微调方法，这里只说个概念）</p><h2 id="DiffFIT微调"><a href="#DiffFIT微调" class="headerlink" title="DiffFIT微调"></a>DiffFIT微调</h2><p>作者选取DiT（DiT-XL&#x2F;2）作为Baseline，DiT（Diffusion Transformer）是基于Transformer的扩散模型。DiffFIT旨在冻结模型所有参数，只训练Bais、Normalization、 class condition module 以及缩放因子$\gamma$。</p><figure>    <img src="DiffFIT.jpg" alt="DiffFIT" style="width:80%;">    <figcaption>图1: DiffFIT原理</figcaption></figure><p>算法流程如下：</p><figure>    <img src="DiffFIT1.jpg" alt="DiffFIT" style="width:80%;">    <figcaption>图2: DiffFIT算法实现</figcaption></figure><p>论文并没有太多原理性论述，作者通过大量的实验来证明以DiffFIT方式微调可以在下游数据集上得到更好的表现。</p><h2 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h2><p>实验所选用的基础模型是DiT-XL&#x2F;2，加载在ImageNet256×256上的预训练7B大小的权重，使用学习率1e-4进行全量微调，评估使用的无分类器引导（cfg）为1.5，cfg为4.0用于可视化。在提出的DiffFIT上学习率放大10倍（1e-3）进行参数高效微调。</p><h3 id="应用于下游数据集"><a href="#应用于下游数据集" class="headerlink" title="应用于下游数据集"></a>应用于下游数据集</h3><p>在Food101, SUN397, DF-20M mini, Caltech101, CUB-200-2011, ArtBench-10, Oxford Flowers and Stanford Cars 8个数据集上微调，并对比全量微调和其他微调方法，通过比较FID来证明作者的微调方法更加高效。作者使用8张V100 GPUs，批大小设置为256，微调迭代240k次进行报告。</p><figure>    <img src="DiffFIT2.jpg" alt="DiffFIT" style="width:80%;">    <figcaption>图3: 下游数据集评测</figcaption></figure><p>尽管全量微调在3&#x2F;8的数据集上表现略好于DiffFIT，但是由于其必须更新所有参数，在时间成本上无法与DiffFIT相比。</p><h3 id="在ImageNet256×256预训练模型上微调-ImageNet-512-×-512数据集"><a href="#在ImageNet256×256预训练模型上微调-ImageNet-512-×-512数据集" class="headerlink" title="在ImageNet256×256预训练模型上微调 ImageNet 512 × 512数据集"></a>在ImageNet256×256预训练模型上微调 ImageNet 512 × 512数据集</h3><p>32个V100 GPUs，批大小1024并且迭代30k次，报告FID分数时采样步长为250。</p><figure>    <img src="DiffFIT3.jpg" alt="DiffFIT" style="width:80%;">    <figcaption>图4: ImageNet上的表现</figcaption></figure><p>从结果中可以看到DiffFIT微调得到了SOTA，FID值为3.02，且用更短的训练时间。</p><hr><p>之后作者在Food512上实验，加载在预训练IN256检查点上微调的Food256的检查点（猜测这个检查点是前面所介绍的下游数据集上的实验），并对比预训练IN512检查点，以及所设计的位置编码技巧。</p><figure>    <img src="DiffFIT5.jpg" alt="DiffFIT" style="width:80%;">    <figcaption>图5: 使用Food101对比所设计的位置编码</figcaption></figure><p>最终，作者发现利用预训练的 ImageNet 512×512 检查点所带来的 FID 性能与预训练并微调的 256×256 的 ImageNet 所实现的性能相似。此外，作者观察到，通过将所提出的位置编码技巧纳入微调过程，FID 性能略有改善。</p><h2 id="收敛性分析"><a href="#收敛性分析" class="headerlink" title="收敛性分析"></a>收敛性分析</h2><p>在上述实验基础上，为了观察模型收敛情况，作者每15k次迭代计算FID分数。</p><figure>    <img src="DiffFIT6.jpg" alt="DiffFIT" style="width:80%;"></figure><p>从图中可以看到，其他微调方法与DiffFIT最终都能有相似的FID，但是DiFFFIT训练更加高效，所需要更新的参数更少（0.12%），更快收敛。</p><p>消融实验中，探索哪些部分的改进对结果有更大的影响。我更关注学习率，作者认为参数的高效微调需要比预训练更大的学习率，因为预训练已经在一定程度上初始化了模型的大部分参数，更大的学习率可以帮助快速适应剩余参数到新的任务。而在实验中作者发现比预训练大10倍的学习率可以产生最佳结果。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>DiffFIT是一种简单高效的微调方法。还是那句话，简单优雅的东西往往能产生令人意外的效果，曾经word2vec论文被ICLR拒稿，时隔十年，被NeurlIPS评为实践检验奖；LORA作者也曾以太过简单为由被拒稿。</p><figure>    <img src="DiffFIT7.jpg" alt="DiffFIT" style="width:80%;"></figure><p>“Most of the things really work is often just simple but elegant.”</p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Diffusion Model</tag>
      
      <tag>Efficient Training</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文解读——DiT|Scalable Diffusion Models with Transformers</title>
    <link href="/blog/2023/12/27/2023/12/27/DiT/"/>
    <url>/blog/2023/12/27/2023/12/27/DiT/</url>
    
    <content type="html"><![CDATA[<h1 id="论文解读——DiT-Scalable-Diffusion-Models-with-Transformers"><a href="#论文解读——DiT-Scalable-Diffusion-Models-with-Transformers" class="headerlink" title="论文解读——DiT|Scalable Diffusion Models with Transformers"></a>论文解读——DiT|Scalable Diffusion Models with Transformers</h1><p><code>ICCV2023</code></p><blockquote><p>摘要：本文是由meta研究人员提出的基于transformer架构的扩散模型，将主干网络（backbone）UNet替换为应用潜空间patches的transformer结构（参考了ViTs网络结构）。主要贡献如下：</p><p>1.全新的扩散模型结构，主干网络替换为transformer结构，并将新的扩散模型称之为DiT。</p><p>2.主要探索：条件模块设计中设计了四种变体、patch size、transformer block architecture、model size。</p></blockquote><h2 id="Diffusion-Transformers"><a href="#Diffusion-Transformers" class="headerlink" title="Diffusion Transformers"></a>Diffusion Transformers</h2><p>DiT的整体结构沿用了潜在扩散模型（LDM），其中VAE、Scheduler等结构并未发生改变，且在训练DiT中加载预训练的模型，在反向去噪过程中，所使用的主干网络UNet被替换为Transformer结构，关于Transformer应用于图像处理请参考ViT模型，DiT主干网络参考了很多ViT的最佳实践。</p><h2 id="四种变体（four-variants-of-transformer-blocks-that-process-conditional-inputs-differently）"><a href="#四种变体（four-variants-of-transformer-blocks-that-process-conditional-inputs-differently）" class="headerlink" title="四种变体（four variants of transformer blocks that process conditional inputs differently）"></a>四种变体（four variants of transformer blocks that process conditional inputs differently）</h2><h3 id="In-context-conditioning"><a href="#In-context-conditioning" class="headerlink" title="In-context conditioning"></a>In-context conditioning</h3><p>这种方式是指将条件信息转化为token直接与输入序列拼接，然后在送入模型训练，这种方式无需修改ViT的结构，使用标准ViT块就可以实现，在最后一个Transformer块之后删除条件信息。</p><figure>    <img src="DiT.jpg" alt="DiT" style="width:80%;">    <figcaption>图1: ViT图像输入</figcaption></figure><p>类似于上图，演示了将图片变成序列并嵌入类别标记。</p><h3 id="Cross-attention-block"><a href="#Cross-attention-block" class="headerlink" title="Cross-attention block"></a>Cross-attention block</h3><p>使用交叉注意力，当使用交叉注意力时，模型不需要将类别标记与时间标记等条件信息嵌入序列，而是把它们作为单独的序列，通过交叉注意力将二者合并。这样的方式条件信息的种类可以更丰富，但是同时也会为模型增加最大的计算量。</p><h3 id="Adaptive-layer-norm-adaLN-block"><a href="#Adaptive-layer-norm-adaLN-block" class="headerlink" title="Adaptive layer norm (adaLN) block."></a>Adaptive layer norm (adaLN) block.</h3><p>主要是说标准层归一化被adaLN取代，通过时间 $t$ 与类别标记 $c$ 的嵌入向量回归得到缩放因子与偏移因子。</p><h3 id="adaLN-Zero-block"><a href="#adaLN-Zero-block" class="headerlink" title="adaLN-Zero block"></a>adaLN-Zero block</h3><p>在UNet中，每个块的最后一个卷积层都会进行0初始化，在实践中发现这样的初始化有利于加速模型收敛，所以在DiT的结构中，作者也对adaLN的参相关参数（缩放因子与偏移因子）进行了0初始化。</p><p>模型的详细配置参考下图，划分出四种不同的模型大小。</p><figure>    <img src="DiT1.jpg" alt="DiT" style="width:80%;">    <figcaption>图2: DiT不同模型尺寸</figcaption></figure><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><p>patches是指将图像按像素级分割，比如p&#x3D;2，图像尺寸为256×256，那么分割后的patches数量为(256&#x2F;&#x2F;2)×(256&#x2F;2)，DiT-XL&#x2F;2表示XLarge型号的模型，patch size是2×2。</p><figure>    <img src="DiT2.jpg" alt="DiT" style="width:50%;">    <figcaption>图3: DiT结构设计</figcaption></figure><p>在ImageNet 数据集 上以 256 × 256 和 512 × 512 图像分辨率上训练类条件潜在 DiT 模型，应用输出线性层0初始化，优化器使用adamW。学习率恒定，设置为1e-4，批量大小为256，唯一使用的数据增强是水平翻转。使用指数滑动平均（EMA）来处理模型权重，在评估阶段，全部使用EMA模型。此外，无其他系数的衰减。依然使用VAE将图像映射到隐空间，VAE加载现成的模型预训练参数，并在训练中保持不变。</p><h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>在评估时使用250个DDPM采样步骤报告FID-50K。</p><p>使用的是ADM的<a href="https://github.com/openai/guided-diffusion">tensorflow评估套件</a>。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><figure>    <img src="DiT3.jpg" alt="DiT" style="width:60%;">    <figcaption>图4: 4种条件策略对比</figcaption></figure><p>训练了四个最高Gflops的DiT-XL&#x2F;2 模型，每个模型都使用不同的块设计。四种条件策略下FID随训练步长的变化情况，可以看到应用adaLN-Zero的模型取得了最优的结果。对于后面所介绍的内容，所有模型都将使用 adaLN-Zero DiT 模块。</p><p>在model size（S、B、L、XL）以及patch size（8、4、2）的空间中缩放模型，作者观察到增加模型大小和减小patch大小可以显着改进扩散模型。</p><figure>    <img src="DiT4.jpg" alt="DiT" style="width:100%;">    <figcaption>图5</figcaption></figure><p>作者认为DiT的Gflops对于提高性能至关重要。图 6 的结果表明参数计数并不能唯一地确定 DiT 模型的质量。当模型大小保持不变并且补丁大小减小时，变压器的总参数实际上没有改变（实际上，总参数略有减少），并且仅 Gflops 增加。这些结果表明，缩放模型的Gflops 实际上是提高性能的关键。为了进一步研究这一点，作者在图 8 中针对模型 Gflops 绘制了 400K 训练步骤下的 FID-50K。结果表明，当总 Gflops 相似时，不同的 DiT 配置会获得相似的 FID 值（例如，DiT-S&#x2F;2 和 DiT- B&#x2F;4）。</p><figure>    <img src="DiT5.jpg" alt="DiT" style="width:60%;">    <figcaption>图6</figcaption></figure><figure>    <img src="DiT6.jpg" alt="DiT" style="width:60%;">    <figcaption>图7</figcaption></figure><p>作者在之后的实验中发现，更大的模型计算效率更高，相对于训练步数较少的大型 DiT 模型而言，小型 DiT 模型即使训练时间较长，最终也会变得计算效率低下。</p><p>之后作者使用最高Gflop的模型DiT-XL&#x2F;2，训练7M步，在ImageNet上达到了SOTA。</p><figure>    <img src="DiT7.jpg" alt="DiT" style="width:80%;">    <figcaption>图8</figcaption></figure><figure>    <img src="DiT8.jpg" alt="DiT" style="width:80%;">    <figcaption>图9</figcaption></figure><p>最后作者还测试了采样步长对于结果的影响。在400k训练步长上，使用[16,,32,64,128,256,1000]的采样步骤绘制FID曲线。</p><figure>    <img src="DiT9.jpg" alt="DiT" style="width:60%;">    <figcaption>图10</figcaption></figure><p>主要得到的结论是扩大采样步长并不能弥补模型计算量所带来的差距。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Diffusion Transformers (DiTs)，这是一种基于 Transformer 的简单扩散模型主干，其性能优于之前的 U-Net 模型，并继承了 Transformer 模型类出色的缩放特性。这将为后续的研究工作奠定基础，期待未来对于Transformer结构下扩散模型的探索。</p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Transformer</tag>
      
      <tag>Diffusion Model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>网络的渐进成长|Deep Growing Learning</title>
    <link href="/blog/2023/12/25/2023/12/25/dgl/"/>
    <url>/blog/2023/12/25/2023/12/25/dgl/</url>
    
    <content type="html"><![CDATA[<h1 id="Deep-Growing-Learning"><a href="#Deep-Growing-Learning" class="headerlink" title="Deep Growing Learning"></a>Deep Growing Learning</h1><p><code>ICCV2017</code></p><blockquote><p>摘要：本篇论文主要研究了半监督学习中模型训练时可能出现的过拟合问题，在深度神经网络上这样的问题尤为严重。文章主要贡献如下：</p><p>1.提出了一种全新的深度生长的学习方法，随着训练的进行，动态的生长新层来提高分类器的能力。</p><p>2.解决了半监督学习中模型过拟合问题，在MNIST和CIFAR-10数据集上表现比以往的方法更优秀。</p></blockquote><p>下面来主要介绍论文中提出的新颖的模型训练方法DGL。</p><h2 id="深度成长学习"><a href="#深度成长学习" class="headerlink" title="深度成长学习"></a>深度成长学习</h2><p>主要设计了四个模块，分别是生长子网络、固定子网络、监督子网络以及选择子网络。</p><figure>    <img src="DGL2.jpg" alt="DGL" style="width:90%;">    <figcaption>图1: DGL结构</figcaption></figure><h3 id="生长子网络"><a href="#生长子网络" class="headerlink" title="生长子网络"></a>生长子网络</h3><p>在生长子网络里提出了“组件”（building blocks），个人觉得类似于块，如果你了解ResNet的话就明白块是什么，文章针对于不同的数据集设计了不同的“组件”。在这里我举一个自己的例子，假设一个“组件”是由两层卷积、一层池化组成。在训练开始时，我们可能会使用较少的块训练，之后随着训练进行，对网络添加新的块，并将前一个块的参数复制到新加入的块中（可以对weights做一些调整），然后对新的网络继续训练，在未达到某一条件之前，网络会不断生长。</p><figure>    <img src="DGL1.jpg" alt="DGL" style="width:40%;">    <figcaption>图1: 增长方式</figcaption></figure><p>对于这篇文章来说，Upsample和残差连接这些都没有，</p><h3 id="固定子网络"><a href="#固定子网络" class="headerlink" title="固定子网络"></a>固定子网络</h3><p>固定子网络是指在经过卷积层之后的全连接层，作者固定了网络中最后的两个全连接层，每一个全连接层都加入dropout和非线性激活层。</p><h3 id="监督子网络"><a href="#监督子网络" class="headerlink" title="监督子网络"></a>监督子网络</h3><p>一个softmax层，该层所计算的结果会反向传播来更新梯度，是实际的模型输出层。</p><h3 id="选择子网络"><a href="#选择子网络" class="headerlink" title="选择子网络"></a>选择子网络</h3><p>一个softmax层，输出one-hot编码的向量，称之为logits。每个位置上的概率值，选出概率最高的作为预测类别。这个模块是为了用无标签数据集生成伪标签筛选出可信标签数据作为有监督学习的数据来训练。（对照模型结构图）</p><p>（半监督学习：首先使用少量标记数据训练模型，然后使用模型对未标记数据进行预测，将预测最自信的部分作为新的训练数据。）</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在半监督任务上，以往的训练方式模型在训练中容易过拟合，本篇论文主要贡献就是提出了深度成长网络，使用这种方式可以很好地解决之前的深度网络在半监督学习上训练过拟合问题。但这一工作为渐进学习做出了重要贡献，为模型训练方式提供了新的方案。往往高效的方式简单而优雅，看似普通的设计都是能够令人振奋的技术变革。</p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DeepLearning</tag>
      
      <tag>Progressive Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>无分类器指导|classifier-free guidance scale</title>
    <link href="/blog/2023/12/23/2023/12/23/cfg/"/>
    <url>/blog/2023/12/23/2023/12/23/cfg/</url>
    
    <content type="html"><![CDATA[<p><a href="https://getimg.ai/guides/interactive-guide-to-stable-diffusion-guidance-scale-parameter">原文链接</a></p><p>简单来说，CFG尺度（无分类器引导尺度）或引导尺度<span style="color: green;">(“CFG scale (classifier-free guidance scale) or guidance scale”)</span>是一个控制图像生成过程遵循文本提示程度的参数。 值越高，图像越贴近给定的文本输入。</p><p>但这并不意味着该值应始终设置为最大值，因为更多的指导意味着更少的多样性和质量。</p><p>当指导比例值设置为1时，文本提示将被忽略。严格遵循最大20，但图像质量较差。 最具“创意”和“艺术性”的结果通常在 7-12 的指导范围内生成。 但使用高达 15 的比例仍然会产生几乎没有artifacts的结果。</p><p>设置正确的值取决于您想要的结果以及文本提示的复杂性。 决定权在于您，但最好总是尝试不同的规模，看看更有创意的结果或严格遵守提示的结果是否更适合您的用例。</p><p>如果您尝试生成提示中指定的更微小细节的图像，则可以从 12 到 16 之间的更高指导比例开始。</p><p>我们希望我们的指南能帮助您了解稳定扩散中的 CFG 比例&#x2F;引导比例参数，并且您将利用所学知识来创造令人惊叹的东西。</p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Diffusion Model</tag>
      
      <tag>Text2Image</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文解读——Denoising Diffusion Probability Model原理与代码分析</title>
    <link href="/blog/2023/12/21/2023/12/21/DDPM/"/>
    <url>/blog/2023/12/21/2023/12/21/DDPM/</url>
    
    <content type="html"><![CDATA[<h1 id="论文解读——Denoising-Diffusion-Probability-Model原理与代码分析"><a href="#论文解读——Denoising-Diffusion-Probability-Model原理与代码分析" class="headerlink" title="论文解读——Denoising Diffusion Probability Model原理与代码分析"></a>论文解读——Denoising Diffusion Probability Model原理与代码分析</h1><p><code>neurips2020</code></p><blockquote><p>摘要：使用扩散模型生成高质量图像，扩散概率模型和朗之万动力学的去噪分数匹配之间的联系设计的加权变分界进行训练来获得最佳结果，在学术界被认为是最早的扩散模型，尽管2015年Jascha Sohl-Dickstein等人已经提出了扩散概率模型。</p><p>论文主要贡献：</p><p>1.作者证明了扩散模型能够生成高质量图像样本。</p><p>2.在假设马尔可夫过程的前提下，作者发现了扩散模型 与变分推理之间的联系，使得可以训练一个优秀的神经网络来生成高质量样本。</p></blockquote><h2 id="DDPM原理"><a href="#DDPM原理" class="headerlink" title="DDPM原理"></a>DDPM原理</h2><p>扩散模型（Diffusion Model）是一种基于物理热力学扩散思想的深度学习生成模型，包括前向扩散和反向扩散两个过程。不太懂的朋友，不妨想象一下，一滴墨水滴入盛满水的杯中逐渐扩散到整个系统的情景。非平衡热力学描述这滴墨水随时间推移的扩散过程中每一个“时间步”状态的概率分布——从一个初始的复杂概率分布逐步扩散变成简单均匀的分布。如果可以反向求出这个过程，那么就可以从简单分布中推导复杂分布。说起扩散模型，甚至可以追溯一下它的前辈——VAE（变分自编码器）和GAN（生成对抗网络），不过这不在本文的讨论范围。</p><p>公认最早的扩散模型DDPM（Denoising Diffusion Probabilistic Model），假设了扩散过程是一个马尔可夫过程（每一个时间步的状态仅有上一个时间步状态的概率分布加上当前时间步的高斯噪声得到），以及扩散过程的逆过程是高斯分布等。</p><p>DDPM的收敛速度与马尔可夫时间步长和样本空间大小（resolution和sample size）成正比。在DDPM中，逆向过程近似于前向过程中添加的高斯噪声；需要迭代所有数千个时间步才能生成一个样本批次。</p><figure>    <img src="DDPM.png" alt="DDPM" style="width:100%;">    <figcaption>图1: DDPM过程</figcaption></figure><p>如图1所示，$x_0$到$x_T$是对数据进行加噪的前向过程，DDPM假设了扩散过程是一个马尔可夫过程（每一个时间步的概率分布仅由上一个时间步状态的概率分布加上当前时间步的高斯噪声得到）。时间步$t-1$到时间步$t$的单步加噪过程的数学表达式如下：<br>$$<br>q(x_t|x_{t-1})&#x3D;\mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_tI)<br>$$</p><p>噪声的方差是一个位于区间$(0,1)$的固定值$\beta_t$确定，均值则由固定值$\beta_t$和当前时刻“带噪”的数据分布确定。</p><p>最终的噪声分布数学表达式如下：<br>$$<br>q(x_{1:T}|x_0)&#x3D;\prod_{t&#x3D;1}^{T}q(x_t|x_{t-1})<br>$$</p><p>设 $\alpha_t:&#x3D;1-\beta_t$，$\bar{\alpha}_{t}:&#x3D;\prod_{s&#x3D;1}^{t}\alpha_s$，则有：<br>$$<br>q(x_t|x_0)&#x3D;\mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I)<br>$$</p><p>$x_t$的公式则为：<br>$$<br>x_t(x_0,\epsilon)&#x3D;\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon<br>$$</p><p>前向过程是将数据噪声化的过程，前向过程的逆过程（反向过程）则是“去噪”的过程，即从随机噪声中迭代恢复出清晰数据的过程。DDPM将反向过程也视为一个马尔可夫链，这个马尔科夫链是由一系列神经网络参数化的高斯分布组成，也就是需要训练的扩散模型UNet。</p><p>从时间步$t$到时间步$t-1$的单步反向“去噪”过程的数学表达式如下：<br>$$<br>p(x_{t-1}|x_t,x_0)&#x3D;\mathcal{N}(x_{t-1};\tilde{\mu}(x_t,x_0),\tilde{\beta}_tI)<br>$$</p><p>UNet网络学习到的weights和bias就是反向过程的均值与方差（假设方差不固定需要学习，DDPM论文中固定了方差但在后续论文的改进中将其变为了可学习的参数），这涉及到了重参数化的思想，在这里简单介绍一下：</p><p>正如 $p(x_{t-1}|x_t,x_0)$所示 ，我们要知道反向过程估计的复杂高斯分布$\mathcal{N}(x;\mu,\sigma)$ 的均值与方差，并在该分布下进行采样，而这样的采样是无法被神经网络所学习的。于是我们从简单高斯分布 $\mathcal{N}(x;0,1)$采样 $z$ ，假设参数 $w$和 $b$ 使得 $\mu$ 与 $\sigma$可以通过UNet网络对$w$ 和 $b$的学习来得到，这样随机性就只存在于$\mathcal{N}(x;0,1)$中，即：<br>$$<br>\mathcal{N}(x;\mu,\sigma)&#x3D;w*\mathcal{N}(x;0,1)+b<br>$$</p><p>这就是<code>重参数化</code>，不过DDPM中固定了方差不变，只学习均值。通过重参数化，$\epsilon_t\sim\mathcal{N}(0, I)$可以得到采样：<br>$$<br>x_t &#x3D; \sqrt{\tilde{\alpha}} x_0+\sqrt{1-\tilde{\alpha}} \epsilon_t<br>$$</p><p>正是因为反向过程的每一步都是参数化的高斯分布，因此可以分别求高斯分布的均值和方差。通过推导，可以得到时间步$t-1$的高斯分布$q(x_{t-1}|x_t,x_0)$的均值与方差的数学表达式：<br>$$<br>\tilde{\beta}_t&#x3D;\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}·\beta_t<br>$$</p><p>$$<br>\tilde{\mu}(x_t,x_0)&#x3D;\frac{\sqrt\alpha_t(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t+\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}x_0<br>$$</p><p>则$x_{t-1}$的公式：<br>$$<br>x_{t-1}&#x3D;\frac{1}{\sqrt{\alpha}_t}(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon)<br>$$</p><p>扩散模型的优化目标数学表达式：<br>$$<br>L^{simple}_{t-1}&#x3D;E_{x_0,\epsilon\sim\mathcal{N}(0,I)}[||\epsilon-\epsilon_\theta(x_t,t)||^2]<br>$$</p><p>在DDIM论文中损失做了一些调整（如下）。现在的扩散模型几乎都采用了DDIM提出了新的采样方式（而非像DDPM一样的马尔可夫过程每一步的采样都依赖上一步的结果），这样的好处是大大提高了采样的效率，之后可能会写一篇关于DDIM的分析。<br>$$<br>\mathcal{L}_{final}&#x3D;\mathcal{L}_{mse}+\lambda\mathcal{L}_{vlb}<br>$$</p><h2 id="扩散过程"><a href="#扩散过程" class="headerlink" title="扩散过程"></a>扩散过程</h2><figure>    <img src="DDPM1.jpg" alt="DDPM" style="width:80%;">    <figcaption>图2: 扩散原理</figcaption></figure><p>一个简单的扩散模型工作的示意图，将输入数据$Z$不断添加高斯噪声，即Diffusion Process来生成带噪数据$Z_T$，之后通过U-Net网络。</p><p>使用U-Net网络你有两种选择，一种是直接预测最后的结果，一种是进行噪声的预测（Diffusers库中实现通过对噪声预测）。根据DIT论文中的表述，DDPM可能选择了对图像（Improvements in DDPMs over the past two years have largely been driven by improved sampling techniques, most notably classifierfree guidance, reformulating diffusion models to predict noise instead of pixels [DDPM] and using cascaded DDPM pipelines where low-resolution base diffusion models are trained in parallel with upsamplers）的预测，此后的研究改进并通过UNet预测噪声来优化网络参数。</p><figure>    <img src="DDPM2.jpg" alt="DDPM" style="width:120%;">    <figcaption>图3: 算法流程</figcaption></figure><p>Algorithm 1展示了完整的训练优化过程，损失经过简化变成一个简单的均方损失函数。Algorithm 2中第4行公式$\epsilon$代表预测噪声，反向过程（马尔科夫过程）需要逐步去噪，$\alpha_t$是时间步调度器（与时间步设置有关），其次是一个$\sigma_tz$，是因为DDPM中每次在去噪后的图像上会再加入一点噪声来增加生成的多样性。</p><figure>    <img src="DDPM3.png" alt="DDPM" style="width:70%;">    <figcaption>图4: 时间步长调度器超参变化曲线</figcaption></figure><p>简单实现扩散模型的训练过程。</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> UNet2DModel<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> DDPMScheduler<span class="hljs-keyword">import</span> torch<span class="hljs-comment"># create model</span>model = UNet2DModel(    sample_size=image_size,    in_channels=<span class="hljs-number">3</span>,    out_channels=<span class="hljs-number">3</span>,    layers_per_block=<span class="hljs-number">2</span>,    block_out_channels=(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">256</span>),    down_block_types=(        <span class="hljs-string">&quot;DownBlock2D&quot;</span>,        <span class="hljs-string">&quot;DownBlock2D&quot;</span>,        <span class="hljs-string">&quot;AttnDownBlock2D&quot;</span>,        <span class="hljs-string">&quot;AttnDownBlock2D&quot;</span>,    ),    up_block_types=(        <span class="hljs-string">&quot;AttnUpBlock2D&quot;</span>,        <span class="hljs-string">&quot;AttnUpBlock2D&quot;</span>,        <span class="hljs-string">&quot;UpBlock2D&quot;</span>,        <span class="hljs-string">&quot;UpBlock2D&quot;</span>,    ),)model.to(device)<span class="hljs-comment"># 设定噪声调度器</span>noise_scheduler = DDPMScheduler(    num_train_timesteps=<span class="hljs-number">1000</span>,    beta_schedule=<span class="hljs-string">&quot;squaredcos_cap_v2&quot;</span>,)<span class="hljs-comment"># 训练循环</span>optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="hljs-number">4e-4</span>)losses = []<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">30</span>):    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dataloader):        clean_images = batch[<span class="hljs-string">&quot;images&quot;</span>].to(device)        <span class="hljs-comment"># add noisy for image</span>        noise = torch.randn(clean_images.shape).to(clean_images.device)        bs = clean_images.shape[<span class="hljs-number">0</span>]        <span class="hljs-comment"># select a random timestep for every image</span>        timesteps = torch.randint(            <span class="hljs-number">0</span>, noise_scheduler.num_train_timesteps, (bs,),            device=clean_images.device        ).long()        <span class="hljs-comment"># Add noise to a clear image based on the noise amplitude at each timestep</span>        noisy_images = noise_scheduler.add_noise(            clean_images,            noise,            timesteps        )        <span class="hljs-comment"># obtain results</span>        noise_pred = model(noisy_images, timesteps, return_dict=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]        <span class="hljs-comment"># compute loss</span>        loss = F.mse_loss(noise_pred, noise)        <span class="hljs-comment"># backward</span>        loss.backward(loss)</code></pre></div><p>实验部分在CIFAR-10和CELEBA-HQ数据集上测试图像的生成能力，评估指标选用了FID与IS，更详细内容可以参考原<a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf">论文</a>。</p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DeepLearning</tag>
      
      <tag>Diffusion Model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文解读——Attention is all you need（理解transformer原理及实现）</title>
    <link href="/blog/2023/12/19/2023/12/19/transformer/"/>
    <url>/blog/2023/12/19/2023/12/19/transformer/</url>
    
    <content type="html"><![CDATA[<h1 id="论文解读——Attention-is-all-you-need（理解transformer原理及实现）"><a href="#论文解读——Attention-is-all-you-need（理解transformer原理及实现）" class="headerlink" title="论文解读——Attention is all you need（理解transformer原理及实现）"></a>论文解读——Attention is all you need（理解transformer原理及实现）</h1><p><code>neurips2017</code></p><p><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">论文地址</a></p><p><a href="https://github.com/Jiawei804/transformer">基于tensorflow2.x的实现</a></p><blockquote><p>摘要：本篇论文是由goole Brain在2017年发表的一篇论文，提出了一个广泛应用于自然语言处理任务的新模型，即Transformer。主要贡献 ：</p><p>1.Transformer是开创性的工作，引领了后来深度学习的发展，截止到2023年，除了在NLP领域，在其他模态下引用Transformer的思想和架构成为主流。</p><p>2.在NLP领域主导的架构是循环神经网络（RNN），Transformer摒弃了RNN的架构，实现了训练并行，提高了训练效率。</p><p>3.提出的自注意力机制（self-Attention）被广泛应用于其他网络，推动了深度学习的发展，是后来BERT、GPT等模型的基础。</p></blockquote><h2 id="Transformer究竟长什么样子？"><a href="#Transformer究竟长什么样子？" class="headerlink" title="Transformer究竟长什么样子？"></a>Transformer究竟长什么样子？</h2><p>话不多说，先上图。</p><figure>    <img src="1.jpg" alt="Transformer" style="width:100%;">    <figcaption style="font-size: 14px;">图1: Transofrmer结构</figcaption></figure><p>先从宏观上介绍，Transformer一共有两部分组成，第一部分是编码器Encoder（左），第二部分是解码器Decoder（右）。位置编码（Positional Encoding）来记录位置信息，Input Embedding、Output Embedding将输入向量转化为Embedding嵌入，然后将二者加起来喂入模型，Decoder最终的输出会通过全连接层和SoftMax得到词表的概率分布，最简单的方式是选出概率最高的词作为候选词。接下来来详细聊一聊内部的设计。</p><h2 id="编码器Encoder"><a href="#编码器Encoder" class="headerlink" title="编码器Encoder"></a>编码器Encoder</h2><p>我们从Inputs一步一步梳理。</p><p>假设我们的训练集只有一句文本，首先要通过分词器tokenizer转变成词id，tokenizer有三种划分方式：分别是char-level、word-level、sub-level。目前更流行的是词根分词（sub-level）。我们假设文本通过tokenizer得到一个$(1,40)$的一个向量，这个就是准备好的Inputs。</p><p>之后，我们将Inputs输入到Embedding层（假设Embedding维度为512），得到一个$(1,50,512)$的词嵌入向量（Embedding中会将Inputs变为one-hot编码之后做矩阵运算（1，50，vocab_size）@（vocab_size，512））。</p><figure>    <img src="2.jpg" alt="Transformer" style="width:100%;">    <figcaption style="font-size: 14px;">图2: Position Embedding</figcaption></figure><p>此论文中的提出的位置编码是确定的值，只与位置有关无需训练得到，pos就是词的位置（这里的词不是word而是sub），$d_{model}$是超参数，这里举例512，维度是$(50,512)$。之后我们将Position Embedding扩展一个维度与Embedding嵌入逐元素相加之后输入到Encoder。</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))</span><span class="hljs-comment"># PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</span><span class="hljs-comment"># pos 和 i</span><span class="hljs-comment"># pos.shape: (seq_len, 1)</span><span class="hljs-comment"># i.shape: (1, d_model)</span><span class="hljs-comment"># PE(pos, i).shape: (seq_len, d_model)</span><span class="hljs-comment"># 计算 pos / 10000^(2i/d_model)</span><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_angles</span>(<span class="hljs-params">pos, i, d_model</span>):    angles = <span class="hljs-number">1</span> / np.power(<span class="hljs-number">10000</span>, (<span class="hljs-number">2</span> * (i // <span class="hljs-number">2</span>)) / np.float32(d_model))    <span class="hljs-keyword">return</span> pos * angles<span class="hljs-comment"># 计算位置信息,sentence_length长度是多少，就有多少个位置要变为位置编码</span><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_position_embedding</span>(<span class="hljs-params">sentence_length, d_model</span>):            angle_rads = get_angles(np.arange(sentence_length)[:, np.newaxis],                            np.arange(d_model)[np.newaxis, :],                            d_model)    <span class="hljs-comment"># sines.shape: [seq_len, d_model / 2]</span>    <span class="hljs-comment"># cosines.shape: [seq_len, d_model / 2]</span>    sines = np.sin(angle_rads[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>])    cosines = np.cos(angle_rads[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>])    <span class="hljs-comment"># position_embedding.shape: [seq_len, d_model]</span>    position_embedding = np.concatenate([sines, cosines], axis=-<span class="hljs-number">1</span>)    <span class="hljs-comment"># 进行维度扩展，把[seq_len, d_model]，变为[1, seq_len, d_model]</span>    position_embedding = position_embedding[np.newaxis, ...]    <span class="hljs-comment"># 变为float32类型 </span>    <span class="hljs-keyword">return</span> tf.cast(position_embedding, dtype=tf.float32)</code></pre></div><figure>    <img src="3.jpg" alt="Transformer" style="width:80%;">    <figcaption style="font-size: 14px;">图3: Position Embedding热力图</figcaption></figure><p>接下来，我们拿一个Encoder举例，N×代表多个Encoder叠加。Encoder的第一个模块是多头注意力（Multi-Head Attention），也是Transformer的核心。</p><h3 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h3><p>多头注意力是由多个缩放点积注意力（自注意力Self-Attention）组成的。</p><figure>    <img src="4.jpg" alt="Transformer" style="width:80%;">    <figcaption style="font-size: 14px;">图4: 缩放点积注意力与注意力计算公式</figcaption></figure><p>多头注意力首先有三个全连接层$W_{Q}$，$W_{K}$，$W_{V}$，Encoder的输入经过这三个全连接层得到$Q、K、V$三个向量，维度都是$(1,50,512)$，之后拆分成8个缩放点积注意力，$q、k、v$维度变为$(1,8,50,64)$。然后计算Attention（公式如上$d_{k}$是$k$的最后一个维度，因为前面的层用的是$d_{model}$所以仍然是512）。每一个 Attention的维度$(1,1,50,64)$，之后再拼接起来就是$(1,8,50,64)$，之后再做维度变换变成$(1,50,512)$。之后做残差连接，然后层归一化（Add &amp; Norm）。</p><p>之后就就进入到一个前馈网络，前馈网络由两个全连接层组成，主要用来做放缩，之后同样做残差连接，然后层归一化（Add &amp; Norm）。最后得到Encoder的输出，也被称为Encoder Hidden State。</p><figure>    <img src="5.jpg" alt="Transformer" style="width:80%;">    <figcaption style="font-size: 14px;">图5: 缩放点积注意力与多头注意力</figcaption></figure><h2 id="解码器Decoder"><a href="#解码器Decoder" class="headerlink" title="解码器Decoder"></a>解码器Decoder</h2><p>解码器与编码器的层结构完全一样，只不过一个解码器有两个注意力层和一个前馈网络层。</p><p>第一个注意力是自己的注意力，第二个注意力是结合了Encoder Hidden State，即Encoder的输出作为</p><p>$K、V$，Decoder的第一个注意力层的输出作为$Q$，然后计算Attention分数。</p><p>最终Decoder模块的输出会经过全连接层和SoftMax来得到词表中每个词的概率值 ，选出概率最高的作为候选值。</p><h2 id="为什么可以实现并行训练？"><a href="#为什么可以实现并行训练？" class="headerlink" title="为什么可以实现并行训练？"></a>为什么可以实现并行训练？</h2><p>在实现细节里，还有很重要的一个部分就是掩码mask，Transformer通过掩码矩阵，使得模型在单步训练中只可以看到它应该看到的信息。在 mask 里，应该被忽略的我们会设成 1，应该被保留的会设成 0。如果 mask 相应位置上为 1，那么我们就给对应的 logits 加上一个超级小的负数， -1e-12， 这样，对应的 logits 也就变成了一个超级小的数。然后在计算 softmax 的时候，一个超级小的数的指数会无限接近与 0。也就是它对应的 attention 的权重就是 0 了。</p><figure>    <img src="6.jpg" alt="Transformer" style="width:80%;">    <figcaption style="font-size: 14px;">图5: mask掩码</figcaption></figure><p>对于Encoder，有两个mask，第一个mask是为了屏蔽掉padding部分，在最开始做数据处理时，需要对文本对齐，比如我们规定文本tokenizer之后的id长度最大为50。另外一个是类似的上三角矩阵。第一个词只能注意到自己，第二个词可以注意到第一个词。第三个词可以注意到前两个词，依次类推。。。</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 1. padding_mask</span><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_padding_mask</span>(<span class="hljs-params">batch_data</span>):    <span class="hljs-string">&quot;&quot;&quot;</span><span class="hljs-string">    :param batch_data: [batch_size, seq_len]</span><span class="hljs-string">    :return: [batch_size, 1, 1, seq_len]</span><span class="hljs-string">    &quot;&quot;&quot;</span>    padding_mask = tf.cast(tf.math.equal(batch_data, <span class="hljs-number">0</span>), dtype=tf.float32)    <span class="hljs-keyword">return</span> padding_mask[:, tf.newaxis, tf.newaxis, :]<span class="hljs-comment"># 2. look_ahead_mask(上三角矩阵，每个词只能注意到前面的没有预知未来的能力)</span><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_look_ahead_mask</span>(<span class="hljs-params">size</span>):    mask = <span class="hljs-number">1</span> - tf.linalg.band_part(tf.ones((size, size)), -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>)    <span class="hljs-keyword">return</span> mask <span class="hljs-comment"># (seq_len, seq_len)</span></code></pre></div><p>对于Decoder，先来说第二个注意力，在$Q*K^T$的运算后得logits，维度是$(…, seq_len_q, seq_len_k)$，logits需要加上Encoder的padding_mask，因为Decoder不需要花费精力去注意Encoder的padding部分。其次来看第一个注意力，第一个注意力与Encoder一样，训练过程中，每一个位置的词不应该看到后面位置的词，以及不应该花时间去注意padding的部分（对于训练而言）。</p><p>个人理解所谓的并行训练就是不需要等待输出结果，每一次Decoder的输入都会使用每个位置正确的词。在推理（预测）中，是不可以并行的，依然需要串行生成。</p><h2 id="学习率warm-up"><a href="#学习率warm-up" class="headerlink" title="学习率warm-up"></a>学习率warm-up</h2><p>这并不是开创性工作，但是这篇论文里用到了这个技术，那就来简单说一说，学习率预热是指在训练的初始阶段，学习率会从一个很小的值逐渐增加，根据设定的超参数决定在哪一步达到设置的值，之后保持不变，这种称之为warm-up_constant，还有很多变种其中包括先上升后下降。</p><figure>    <img src="7.jpg" alt="Transformer" style="width:60%;">    <figcaption style="font-size: 14px;">图5: warm-up学习率预热曲线</figcaption></figure><h2 id="机器翻译的评价指标——bleu"><a href="#机器翻译的评价指标——bleu" class="headerlink" title="机器翻译的评价指标——bleu"></a>机器翻译的评价指标——bleu</h2><p>论文里作者着眼NLP领域的机器翻译任务，采用bleu评价指标，在这里做一个介绍。</p><h3 id="什么是bleu？"><a href="#什么是bleu？" class="headerlink" title="什么是bleu？"></a>什么是bleu？</h3><p>BLEU（Bilingual Evaluation Understudy）是一种用于自动评估机器翻译质量的指标。它最初由Kishore Papineni等人在2002年提出，旨在解决人工评估翻译质量的主观性和费时性问题。BLEU的工作原理是将机器生成的翻译与人工参考翻译进行比较，并根据它们之间的相似性分配一个分数。我们可以通过微软的nltk框架来使用bleu指标。</p><p>BLEU的计算方式如下：</p><ol><li><p>对于翻译结果（机器生成的翻译），它会计算参考翻译（目标值）之间的 n-gram（连续的 n 个词或字符序列）匹配度。</p></li><li><p>对每个 n-gram 匹配，BLEU将其与候选翻译中的 n-gram 数量相比较，并采用一种修正的精确匹配度度量来计算得分。</p></li><li><p>然后，BLEU将各个 n-gram 匹配的得分合并，通过计算几何平均值来得出最终的 BLEU 分数。通常，BLEU 的分数在0到1之间，表示翻译的质量，越接近1表示越好。</p></li></ol><p><strong>公式如下：</strong><br>$$<br>bleu &#x3D; exp^{\sum weight*logP}*惩罚系数<br>$$</p><h3 id="什么是n-gram？"><a href="#什么是n-gram？" class="headerlink" title="什么是n-gram？"></a>什么是n-gram？</h3><div class="code-wrapper"><pre><code class="hljs">与其解释抽象的公式，不如来一段代码实战理解，请看下面这段代码。</code></pre></div><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.translate.bleu.score <span class="hljs-keyword">import</span> sentence_bleureference = [[<span class="hljs-string">&#x27;the&#x27;</span>,<span class="hljs-string">&#x27;quick&#x27;</span>,<span class="hljs-string">&#x27;brown&#x27;</span>,<span class="hljs-string">&#x27;fox&#x27;</span>,<span class="hljs-string">&#x27;jumped&#x27;</span>,<span class="hljs-string">&#x27;over&#x27;</span>,<span class="hljs-string">&#x27;the&#x27;</span>,<span class="hljs-string">&#x27;lazy&#x27;</span>,<span class="hljs-string">&#x27;dog&#x27;</span>]]candidate = [<span class="hljs-string">&#x27;the&#x27;</span>,<span class="hljs-string">&#x27;quick&#x27;</span>,<span class="hljs-string">&#x27;brown&#x27;</span>,<span class="hljs-string">&#x27;fox&#x27;</span>,<span class="hljs-string">&#x27;jumped&#x27;</span>,<span class="hljs-string">&#x27;over&#x27;</span>,<span class="hljs-string">&#x27;the&#x27;</span>,<span class="hljs-string">&#x27;lazy&#x27;</span>,<span class="hljs-string">&#x27;dog&#x27;</span>]score = sentence.bleu(reference, candidate)<span class="hljs-built_in">print</span>(score)<span class="hljs-comment"># out: 1.0</span><span class="hljs-comment"># 4-gram cumulative BLEU</span>candidate = [<span class="hljs-string">&#x27;the&#x27;</span>,<span class="hljs-string">&#x27;fast&#x27;</span>,<span class="hljs-string">&#x27;brown&#x27;</span>,<span class="hljs-string">&#x27;fox&#x27;</span>,<span class="hljs-string">&#x27;jumped&#x27;</span>,<span class="hljs-string">&#x27;over&#x27;</span>,<span class="hljs-string">&#x27;the&#x27;</span>,<span class="hljs-string">&#x27;lazy&#x27;</span>,<span class="hljs-string">&#x27;dog&#x27;</span>]score = sentence_bleu(reference, candidate, weights=(<span class="hljs-number">0.25</span>, <span class="hljs-number">0.25</span>, <span class="hljs-number">0.25</span>, <span class="hljs-number">0.25</span>))<span class="hljs-built_in">print</span>(score)<span class="hljs-comment"># out:0.75</span><span class="hljs-comment"># 手动计算</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npscore = np.exp(<span class="hljs-number">0.25</span> * (np.log(<span class="hljs-number">8</span>/<span class="hljs-number">9</span>) + np.log(<span class="hljs-number">6</span>/<span class="hljs-number">8</span>) + np.log(<span class="hljs-number">5</span>/<span class="hljs-number">7</span>) + np.log(<span class="hljs-number">4</span>/<span class="hljs-number">6</span>)))<span class="hljs-built_in">print</span>(score)<span class="hljs-comment"># out:0.75</span></code></pre></div><ul><li><p>过短惩罚系数，上例预测结果与目标结果长度一致，故为1。</p><ul><li>当预测结果比目标结果短时，过短惩罚系数就不为1了。</li></ul></li><li><p>在实际中，参考句可能不止一句，所以reference是二维。</p><ul><li>实际上，只要reference比candidate多一个维度就可以，candidate未必是一维的。</li></ul></li><li><p>n-gram实际上就是连续n个词一样。</p></li><li><p>weights是对每个gram赋予一个权重。</p></li><li><p>4-gram考查了1个，2个，3个，4个连续的词相同的情况。</p></li><li><p>P是连续n个词相同的概率。</p></li><li><p>bleu一定是一个0-1之间的值。</p><ul><li>P是0-1之间，logP是负数，乘以weights仍然是负数。</li><li>e的负数次幂在0-1之间。</li><li>过短惩罚系数是一个0-1之间的值。</li></ul></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以上是论文的主要贡献，开创性的Transformer架构成为了深度学习划时代的标志，像此后的GPT模型就是采用Decoder模块堆叠，他们的底层原理都来自于这一工作。谈到GPT3.5的成功，其实并不是谁都可以做的这么好，这是OpenAI在算法，工程和系统问题上的集大成之作。</p><p>在知乎等各种平台上，我们可以看到很多解读这篇论文的工作，作为一名在深度学习领域学习探索的学生，都应该认真学习这篇跨时代意义的论文。</p><p>“Don’t dwell on the past, the moments take  our breath away will actually measure the life.”</p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DeepLearning</tag>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高中|17级18班</title>
    <link href="/blog/2023/12/18/2023/12/18/high_school/"/>
    <url>/blog/2023/12/18/2023/12/18/high_school/</url>
    
    <content type="html"><![CDATA[<hr><div class="row justify-content-sm-center">    <div class="col mt-3 mt-md-0">        <img src="/blog/2023/12/18/2023/12/18/high_school/13.jpg" class="" title="高中去向">    </div></div><hr>]]></content>
    
    
    <categories>
      
      <category>沉思录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>thoughts</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + HEXO学习之旅</title>
    <link href="/blog/2023/12/13/2023/12/13/use_hexo/"/>
    <url>/blog/2023/12/13/2023/12/13/use_hexo/</url>
    
    <content type="html"><![CDATA[<div style="text-align: center;">     <h1>HEXO学习之旅</h1> </div><p>在最开始的时候，计划采用购买国内服务器来搭建站点，不过一直都没有实施，懒狗石锤，就是想的多做得少。不过后来在知乎上看到了一个回答，结识了一位同学，说是可以使用GitHub Pages+HEXO来搭建，于是我就开始了我的GitHub Pages + HEXO之旅。</p><h2 id="GitHub-Pages"><a href="#GitHub-Pages" class="headerlink" title="GitHub Pages"></a>GitHub Pages</h2><p>GitHub Pages是GitHub提供的一个静态网页托管服务，可以直接使用GitHub的仓库来托管网页，而且还可以绑定自己的域名，这样就可以使用自己的域名来访问网页了。</p><p>不过这一过程花费了我很长的时间，我第一次真正的下水实操配置域名，实现多域名解析，还有授权github actions等等，失败，失败，失败……，差一点放弃，不过最终还是成功了。这个过程很奇妙，我怀着热情，每天在晚上抽出几个小时，有时我会干到很晚，经常实验室就剩我一个人。</p><p>完成github pages以及github actions，主页就初步建成了。</p><img src="/blog/2023/12/13/2023/12/13/use_hexo/photo.jpg" class="" title="学院石碑"><h2 id="令人兴奋的开源项目"><a href="#令人兴奋的开源项目" class="headerlink" title="令人兴奋的开源项目"></a>令人兴奋的开源项目</h2><p>如果是我一个人手撕这个网站，那基本是猴年马月了，说真的，我对开源项目有崇高的敬意。还有那些热爱分享的年轻人，我的出发是一位网友引领的，我们从未相见过，只是因为在一篇帖子中请教问题，他回复了我。当我访问他的主页时，看到了他的博客，我之前一直有想法建一个属于自己的网页，于是乎我抱着尝试的心态询问了他。</p><p>我并没有想到他会回复我，他真的私信了我，令我感激的是，后来我遇到了一些麻烦，他依然愿意为我解答。这种感觉很棒，我想起linux课上，老师在黑板上写的一个单词”free“，他说，”这个单词不只是免费的意思，它代表着开源精神“。当然，开源并不等同于免费，我想我便爱上了开源。</p><h2 id="HEXO"><a href="#HEXO" class="headerlink" title="HEXO"></a>HEXO</h2><p>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他标记语言）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。这个框架我以前从未接触过，我打开官方网站，开始按着教程一步一步实践。我在本地搭建了一个Hexo，然后在本地写了一篇文章，部署到了GitHub Pages上，这个过程并不顺利，还好，现在你们看到了，它就在这里。”What you see is what you get“.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我很享受这样的过程，痛苦、兴奋、尝试，这些属于我自己的时间，总有一天会闪闪发光。当我现在回头再看，发现这些流程是如此简单，可能真的是这样的，当你身在局中的时候，反而看不到本质的东西。行文至此，该结束了，下面放一段最近很喜欢的话吧。</p><p>"Three passions, simple but overwhelmingly strong, have governed my life: the longing for love, the search for knowledge, and unbearable pity for the suffering of mankind. These passions, like great winds, have blown me hither and thither, in a wayward course, over a great ocean of anguish, reaching to the very verge of despair."</p> <p style="text-align:right">--罗素</p>]]></content>
    
    
    <categories>
      
      <category>沉思录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>thoughts</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
