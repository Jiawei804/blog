<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2023年度AI事件</title>
    <link href="/blog/2023/12/30/2023/12/30/AI%E5%A4%A7%E4%BA%8B%E4%BB%B6/"/>
    <url>/blog/2023/12/30/2023/12/30/AI%E5%A4%A7%E4%BA%8B%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="年度AI事件"><a href="#年度AI事件" class="headerlink" title="年度AI事件"></a>年度AI事件</h1><p>毫无疑问，2022年11月30日ChatGPT的发布，代表着人工智能新一轮的热潮，而2023年的这些故事将会塑造一个全新的未来：</p><h2 id="人工智能的进展"><a href="#人工智能的进展" class="headerlink" title="人工智能的进展"></a>人工智能的进展</h2><p>在今年的人工智能领域，可谓是神仙打架，国外的微软、谷歌、Meta等互相比拼，国内也爆发白模大战。这一年中，文生图以及文生视频等赛道都孵化出了令人惊艳的产品。</p><h3 id="图像生成"><a href="#图像生成" class="headerlink" title="图像生成"></a><strong>图像生成</strong></h3><ul><li><strong>Midjourney：</strong> <a href="https://journal.everypixel.com/top-ai-news-march-2023">Midjourney 的 V.5 模型</a>标志着图像生成领域的一个里程碑，展示了更高的效率、一致性和更高的分辨率。而最近最新的<a href="https://mid-journey.ai/midjourney-v6-release/">Midjourney V.6</a>带来了额外的增强功能，例如更准确的提示跟随、增加的模型知识和较小的文本绘制能力。</li></ul><figure>    <img src="1.jpg" alt="CLIP" style="width:50%;">    <figcaption>(上v6，下v5）</figcaption></figure>  <ul><li><strong>DALL·E 3：</strong><a href="https://journal.everypixel.com/top-ai-news-september-2023">DALL·E 3</a> 基于 ChatGPT 构建， 简化了图像生成，无需复杂的提示工程。此外，ChatGPT还引入了一项功能，可以帮助用户完善提示并根据反馈进行图像调整。</li></ul><figure>    <img src="2.jpg" alt="CLIP" style="width:80%;">    <figcaption>(左DELL·2，右DELL·3）</figcaption></figure><h3 id="视频生成"><a href="#视频生成" class="headerlink" title="视频生成"></a><strong>视频生成</strong></h3><ul><li><strong>Stability AI：</strong> Stability AI <a href="https://journal.everypixel.com/top-ai-news-november-2023">推出了 Stable Video Diffusion</a>，这是一种开创性的生成视频模型，在 GitHub 开源可访问。<a href="https://journal.everypixel.com/ai-image-statistics">与人工智能图像生成趋势</a>相似 ，稳定视频扩散模型很可能在大部分人工智能生成视频的创建中发挥关键作用。</li><li><strong>HeyGen：</strong> 人工智能初创公司推出了 <a href="https://the-decoder.com/heygen-offers-ai-powered-video-translation-with-impressive-lip-syncing-capabilities/">一款用于语音克隆</a>、嘴唇运动调整和视频语言翻译的工具。</li><li><strong>Runway Gen-2</strong>： <a href="https://research.runwayml.com/gen2">Runway 推出了 Gen-2</a> 模型，使用户能够轻松地从文本提示、图像或其他视频生成完整的视频。看看下面的例子。 </li><li><strong>Pika 和 Pika 1.0</strong>：随着最初的发布，Pika 获得了 50 万用户，每周生成数百万个视频。<a href="https://pika.art/launch">随后Pika 1.0</a>中升级的AI模型 使用户能够创建和编辑各种风格的视频，包括3D动画、动漫、卡通和电影。</li><li><strong>Meta 的编解码器头像：</strong> <a href="https://youtu.be/MVYrJJNdrEg?si=NR3DJMeYOfbiAunX">Meta 的</a> 用于视频中 3D 人脸的像素编解码器头像 (PiCA) 模型更逼真的呈现效果。</li></ul><h3 id="文本生成"><a href="#文本生成" class="headerlink" title="文本生成"></a><strong>文本生成</strong></h3><p><strong>文本生成</strong></p><ul><li><strong>Bard 和 Gemini：</strong> <a href="https://journal.everypixel.com/top-ai-news-march-2023">Google 的 Bard</a> 在聊天机器人领域添加了类人的情感和情绪。<a href="https://blog.google/technology/ai/google-gemini-ai/">谷歌的 Gemini</a>引入巴德聊天机器人并在多模态数据集上进行训练， 成为“最有能力”的人工智能模型，也是 OpenAI 的 ChatGPT 最接近的竞争对手。</li><li><strong>Grok：</strong> <a href="https://journal.everypixel.com/top-ai-news-april-2023">Elon Musk 的初创公司 xAI通过</a><a href="https://journal.everypixel.com/top-ai-news-november-2023">推出“Grok”</a> ——一个幽默、叛逆、通过 𝕏 平台提供实时知识的聊天机器人， 标志着对人工智能开发的承诺，并有可能与 OpenAI 竞争 。xAI 承诺 Grok<a href="https://x.ai/">旨在回答</a>其他人工智能系统拒绝的挑衅性问题。</li><li><strong>OverflowAI：</strong> <a href="https://journal.everypixel.com/top-ai-news-july-2023">Stack Overflow 的 OverflowAI</a> 增强了知识管理，支持在 Visual Studio Code 和 Slack 中通过 AI 搜索相关答案。</li><li><strong>Llama 2：</strong> <a href="https://journal.everypixel.com/top-ai-news-july-2023">Meta 发布了 Llama 2</a>，这是其下一代开源大型语言模型，展示了增强的效率。Meta 经过微调的 LLM 还针对对话用例进行了优化，并且在大多数基准测试中都优于其他开源模型。</li><li><strong>GPT-4：</strong> <a href="https://journal.everypixel.com/top-ai-news-march-2023">OpenAI 的 GPT-4</a> 现在可以处理图像输入、生成字幕、分类、在来回对话中收听和响应，并支持 <a href="https://journal.everypixel.com/top-ai-news-september-2023">实时网页浏览</a>。OpenAI 还扩展了对插件的支持，促成了一个充满开源竞争对手的环境。GPT-4 是 OpenAI 开发 AGI 之旅的下一步。</li><li><strong>Mistral 7B：</strong> <a href="https://mistral.ai/">Mistral AI</a> 今年<a href="https://www.nytimes.com/2023/12/10/technology/mistral-ai-funding.html">估值约20亿美元</a>， 发布了Mistral 7B，这是一个挑战GPT-4和Claude 2的大型语言模型。Mistral AI强调开放的技术方法，提供免费下载其模型。</li><li><strong>Mixtral 8x7B：</strong> <a href="https://mistral.ai/news/mixtral-of-experts/">Mistral AI 还推出了 Mixtral 8x7B</a>，这是一种具有开放权重的高质量稀疏混合专家模型 (SMoE)，具有 46.7B 总参数，开创了模型的开放性，增强了真实性并减少了偏差。</li><li><strong>Yi-34B llm：</strong> 今年 <a href="https://techcrunch.com/2023/11/05/valued-at-1b-kai-fu-lees-llm-startup-unveils-open-source-model/">估值为 10 亿美元，李开复的初创公司</a><a href="http://01.ai/">01.AI</a> 发布了 Yi-34B，这是一个开源的模型，其参数数量显着高于竞争模型，强调了其成本效益。尽管之后陷入抄袭风波，但不可否认零一万物的成功。</li></ul><h2 id="国内外事件时间线"><a href="#国内外事件时间线" class="headerlink" title="国内外事件时间线"></a>国内外事件时间线</h2><ul><li><p>2022年11月30日，chatGPT问世。</p></li><li><p>2023年2月1日，chatGPT plus版本上线，OpenAI开启订阅付费计划。</p></li><li><p>2023年2月7日，微软将chatGPT功能集成到New Bing，当日微软市值暴涨 800 亿美元。</p></li><li><p>2023年2月7日，谷歌Bard首秀Demo并翻车，股票一夜之间暴跌 7000 亿人民币。</p></li><li><p>2023年2月24日，Meta发布LLaMa 并开源，带动了开源社区和AI社区的高速成长和发展。</p></li><li><p>2023年3月1日，OpenAI推出ChatGPT API，供开发者集成。</p></li><li><p>2023年3月14日 OpenAI发布GPT-4,并在ChatGPT和Bing中支持。</p></li><li><p>2023年3月14日，斯坦福发布Alpaca，一个由LLaMA 7B微调的大模型。</p></li><li><p>2023年3月16日 微软推出copilot（AI智能助手），重塑未来的生产方式。</p></li><li><p>2023年3月16日，百度发布文心一言，此后进入国内的百模大战。</p></li><li><p>2023年3月17日，微软GPT-4 Office全家桶发布。</p></li><li><p>2023年3月21日，Midjourney v5版本画出100%逼真情侣。</p></li><li><p>2023年3月22日，Runway 重磅发布Gen-2，文生视频里程碑。</p></li><li><p>2023年3月24日，OpenAI 为解除 ChatGPT 无法联网的限制，启动第三方插件支持功能。</p></li><li><p>2023年3月29日，千名大佬发联名信，叫停GPT-5超强大模型。</p></li><li><p>2023年3月31日，意大利暂时禁止ChatGPT使用，于4月28日恢复ChatGPT服务。</p></li><li><p>2023年4月6日，Meta发布可以分割一切的Segment Anything。</p></li><li><p>2023年4月20日，Google Brain与DeepMind 合并成立 Google DeepMind。</p></li><li><p>2023年5月5日，微软BingChat全面开放。</p></li><li><p>2023年7月13日，马斯克官宣成立xAI。</p></li><li><p>2023年7月19日，Llama 2开源可商用，模型系列包含 70 亿、130 亿和 700 亿三种参数。</p></li><li><p>2023年 8月10日，斯坦福「虚拟小镇」开源，引爆智能体（AI Agent）研究。</p></li><li><p>2023年8月29日，OpenAI发布企业版ChatGPT：没有限制、更快、更强、更安全的GPT-4。</p></li><li><p>2023年9月21日，OpenAI推出DALL·E 3，并将原生集成至ChatGPT中。 </p></li><li><p>2023年10月17日，文心大模型4.0发布。</p></li><li><p>2023年10月20日，ChatGPT全球宕机，API崩溃。</p></li><li><p>2023年10月29日，GPT-4集成图像上传 + 插件 + 代码运行器 + 文件上传 + 图像生成。</p></li><li><p>2023年11月15日，奥特曼被OpenAI董事会开除系列事件。</p></li><li><p>2023年11月29日 文生视频产品Pika 1.0正式发布。</p></li><li><p>2023年12月6日，谷歌DeepMind发布Gemini系列模型。</p></li><li><p>2023年12月14日，谷歌官宣开放Gemini API，奥特曼宣布ChatGPT Plus恢复订阅。</p></li><li><p>2023年12月21日，MidJounery V6 发布。</p></li></ul><hr><span style="color: gray;">参考文章：  <p><a href="https://journal.everypixel.com/2023-the-year-of-ai">https://journal.everypixel.com/2023-the-year-of-ai</a><br><a href="https://mp.weixin.qq.com/s/kpb86Nap6sqct6HqrtJc0g">https://mp.weixin.qq.com/s/kpb86Nap6sqct6HqrtJc0g</a><br></span></p>]]></content>
    
    
    <categories>
      
      <category>沉思录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文解读——CLIP|Learning Transferable Visual Models From Natural Language Supervision</title>
    <link href="/blog/2023/12/29/2023/12/29/CLIP/"/>
    <url>/blog/2023/12/29/2023/12/29/CLIP/</url>
    
    <content type="html"><![CDATA[<h1 id="论文解读——CLIP-Learning-Transferable-Visual-Models-From-Natural-Language-Supervision"><a href="#论文解读——CLIP-Learning-Transferable-Visual-Models-From-Natural-Language-Supervision" class="headerlink" title="论文解读——CLIP|Learning Transferable Visual Models From Natural Language Supervision"></a>论文解读——CLIP|Learning Transferable Visual Models From Natural Language Supervision</h1><blockquote><p>摘要：本篇论文是OpenAI在多模态领域的成果之一，CLIP（Contrastive LanguageImage Pre-training）连接了文本和图像，旨在将同一语义的文字和图片转化到同一个隐空间下。论文主要贡献：</p><p>1.论文使用在互联网上收集的4亿个图像文本对的数据集从头训练一个模型，这种预测标题（文本）与图像的对应的预训练更像是一种广泛的监督学习，高效且可扩展。</p><p>2.在预训练好的模型上，能够实现zero-shot学习应用于下游任务。例如，在 ImageNet 零样本上可以媲美标准 ResNet50 的准确性（分类任务），而无需使用其所训练的 128 万个训练样本中的任何一个。</p><p>3.CLIP推动了基于文字引导的文字生成图像技术在图像生成领域的快速发展，是后续DALLE、Imagen、stable diffusion的基石。</p></blockquote><h2 id="CLIP如何将图像与文本映射到同一潜空间？"><a href="#CLIP如何将图像与文本映射到同一潜空间？" class="headerlink" title="CLIP如何将图像与文本映射到同一潜空间？"></a>CLIP如何将图像与文本映射到同一潜空间？</h2><p>在最开始，作者模仿之前类似的工作，通过训练一个图像CNN和一个文本Transformer来预测图像标题，但是很难继续下去，并且Transformer模型的计算量已经是原始ResNet50的两倍之多。</p><p>随后作者在表示学习的启发下，考虑探索一个系统，能够预测文本与图像两个整体的对应关系而不是文本的确切单词与图像的对应。事实证明，这样的方式效率提升了4倍。</p><p>作者设计了一种训练方式：给定$N$个（img，text），CLIP被训练并预测在$N×N$种组合中哪一种实际发生。</p><p>为了达到这个目标，作者使用了一个图像编码器（image encoder）和一个文本编码器（text encoder），如下图。</p><figure>    <img src="CLIP1.jpg" alt="CLIP" style="width:80%;">    <figcaption>图1: （text,image) encoder</figcaption></figure><p>之后，将图像和文本转化为embedding并最大化$N$个真实配对的图像文本之间的余弦相似度，同时最小化$N^2-N$个不正确配对的图像文本之间的余弦相似度。优化目标是这些相似性分数的平均交叉熵损失。</p><p>下面的伪代码先将图像、文本转化为嵌入向量，之后计算出图像和文本的余弦相似度（logits），最后分别计算图像交叉熵和文本的交叉熵，取二者的平均作为损失。</p><figure>    <img src="CLIP2.jpg" alt="CLIP" style="width:80%;">    <figcaption>图2: 计算余弦相似度</figcaption></figure><p>值得一提的是，训练过程的唯一数据增强是对Resized的图像进行随机方形裁剪。对于Softmax中控制对数范围的温度参数$\tau$将其转化为了对数参数化的乘法标量避免其作为超参数。</p><h2 id="CLIP模型结构设计"><a href="#CLIP模型结构设计" class="headerlink" title="CLIP模型结构设计"></a>CLIP模型结构设计</h2><p>对于图像编码器，作者考虑了两种主流架构。第一种是使用ResNet50，将全局平均池化替换为注意力池化机制，注意力池化层只使用一个简单的Transformer样式的多头QKV（query、key、value）注意力，query是图像的全局平均池化表示。第二种是ViT（Vision Transformer），在此模型上作者只做了微小修改，在Transformer输入之前的combined patch和position embedding后加了一个额外的归一化层，以及不同的初始化模型权重的方法。对于文本编码器，作者使用12层宽度为512的8头注意力Transformer。CLIP的整体表现对文本编码器并不敏感。</p><h2 id="Using-CLIP"><a href="#Using-CLIP" class="headerlink" title="Using CLIP"></a>Using CLIP</h2><p>用预训练好的CLIP在下游数据集上测试其零样本迁移学习的能力。对于每一个数据集，使用每个类的名字构造出文本，过程如下。</p><figure>    <img src="CLIP3.jpg" alt="CLIP" style="width:80%;">    <figcaption>图3: 应用于下游数据集</figcaption></figure><p>在27个下游数据集上评估，CLIP的零样本分类能力与标准ResNet50的完全监督线性分类器（指最后分类任务使用logistic还是linear）对比，发现在16个数据集上CLIP的表现都更加优秀，其中包括ImageNet。</p><figure>    <img src="CLIP4.jpg" alt="CLIP" style="width:50%;">    <figcaption>图4</figcaption></figure><p>在STL10数据集上作者发现CLIP可以得到SOTA性能（99.3%），而在细粒度数据集上，其中Food101和Stanford Cars上的CLIP性能比使用逻辑回归的ResNet50超过20%，而在Flowers 102和FGVCAircraft上，zero-shot CLIP的表现则低于10%以上。</p><p>Zero-shot CLIP在两个测量视频动作识别的数据集上的表现明显优于ResNet 50。在Kinetics 700上，CLIP的表现比ResNet 50高出14.5%。Zero-shot CLIP在UCF 101上的表现也比ResNet 50高出7.7%。作者推测CLIP能够从文本中提取更多的动态语义而分类只是名词（单个单词）。</p><p>Zero-shot CLIP在面对专门的、复杂的或抽象的任务时表现欠佳。例如卫星图像分类（EuroSAT和RESISC 45）、淋巴结肿瘤检测（PatchCamelyon）、计算合成场景中的物体（CLEVRCounts），以及自动驾驶相关任务，例如德国交通标志识别（GTSRB）、识别与最近汽车的距离（KITTI Distance）。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>CLIP尽管能够与ResNet-50的baseline竞争，但这并不是意味着它达到了SOTA，所以后续仍然有大量的工作需要进行来改进CLIP的任务学习能力与迁移学习能力。</p><p>聊一聊对于文本生成图像的贡献。实际上，通过文本引导图像生成的概念并非首次出现在CLIP中。在CLIP之前，就已经有研究工作在探索这一领域，例如 GAN（生成对抗网络）相关的研究，2014 年的条件GAN（Conditional GAN）和后续的文本到图像的生成模型。CLIP，由OpenAI在2021年提出，并且它对于这个领域的贡献显著。</p><ul><li><p>强大的跨模态理解<br>CLIP通过在大规模数据集上预训练来学习图像和文本之间的关联。这使得它能够理解和捕捉到丰富的、多样化的视觉概念和语言描述之间的关系。</p></li><li><p>零样本学习能力<br>CLIP展示了出色的零样本学习能力，即在没有看过特定任务的训练样本的情况下，它仍能执行该任务。这意味着CLIP可以理解并响应以前未见过的文本描述，生成或识别与之相关的图像内容。</p></li><li><p>开辟新应用<br>CLIP的出现推动了一系列新的应用和研究，例如基于文本的图像搜索、内容创建和图像理解等。它为后续的文本到图像的生成模型（如DALL·E）铺平了道路，这些模型能够更直接地将文本描述转换为图像。</p></li><li><p>新的研究方向<br>CLIP的设计和成功推动了对跨模态（如文本和图像）学习和表示的进一步研究。它展示了通过对齐和联合训练两种不同类型的数据（图像和文本）来构建更为强大和通用的模型的潜力。</p></li><li><p>高效的标注效率<br>传统上，许多机器学习模型依赖大量标注的数据。CLIP通过学习直接从自然语言描述中获取监督信息，显示了减少对手工标注数据依赖的可能性。</p></li></ul><p>总而言之，虽然CLIP不是第一个文本引导图像生成的项目，但它在理解跨模态关系、零样本学习、以及推动文本到图像生成领域的发展方面做出了重要的贡献。</p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>zero-shot</tag>
      
      <tag>transfer learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文解读——DiffFit|Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning</title>
    <link href="/blog/2023/12/28/2023/12/28/DiffFIT/"/>
    <url>/blog/2023/12/28/2023/12/28/DiffFIT/</url>
    
    <content type="html"><![CDATA[<h1 id="论文解读——DiffFit-Unlocking-Transferability-of-Large-Diffusion-Models-via-Simple-Parameter-Efficient-Fine-Tuning"><a href="#论文解读——DiffFit-Unlocking-Transferability-of-Large-Diffusion-Models-via-Simple-Parameter-Efficient-Fine-Tuning" class="headerlink" title="论文解读——DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning"></a>论文解读——DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning</h1><blockquote><p>摘要：本文提出了一种高效的文本生成图像模型的微调方法，命名为DiffFIT，在8个下游数据集上的测试都优于现有的微调策略。本文的主要贡献：</p><p>1.提出了DiffFIT的微调方法，旨在已有预训练DiT模型参数的基础上，微调训练0.12%的参数，在下游数据集上取得了比以往微调方式高效的结果。</p><p>2.通过直观的理论分析以及设计的消融实验，表明为什么简单的参数高效微调可以快速适应新的分布。</p></blockquote><h2 id="参数微调方法"><a href="#参数微调方法" class="headerlink" title="参数微调方法"></a>参数微调方法</h2><h3 id="直接微调"><a href="#直接微调" class="headerlink" title="直接微调"></a>直接微调</h3><p>不改变模型结构的基础上 ，加载预训练模型并在有限的数据集上微调训练，通常选取更小的学习率，如全量微调，冻结等技术。</p><h3 id="适应性微调（Adaptive-Fine-Tuning）"><a href="#适应性微调（Adaptive-Fine-Tuning）" class="headerlink" title="适应性微调（Adaptive Fine-Tuning）"></a>适应性微调（Adaptive Fine-Tuning）</h3><p>通过增加一些小的结构，在预训练模型权重的基础上，微调小的结构上的参数， 而预训练参数全部冻结，通过这种方式调整模型的行为。如LoRA在自注意力子层之间的q和v结果中添加了两个低秩矩阵进行微调。</p><h3 id="提示微调（Prompt-Tuning）"><a href="#提示微调（Prompt-Tuning）" class="headerlink" title="提示微调（Prompt Tuning）"></a>提示微调（Prompt Tuning）</h3><p>P-Tuning在模型输入上调整输入序列的内容，加入一些可训练的提示或标志，以此来改善模型在有提示下的特定任务上的表现。</p><p>（后续或许会单独写一下这些微调方法，这里只说个概念）</p><h2 id="DiffFIT微调"><a href="#DiffFIT微调" class="headerlink" title="DiffFIT微调"></a>DiffFIT微调</h2><p>作者选取DiT（DiT-XL&#x2F;2）作为Baseline，DiT（Diffusion Transformer）是基于Transformer的扩散模型。DiffFIT旨在冻结模型所有参数，只训练Bais、Normalization、 class condition module 以及缩放因子$\gamma$。</p><figure>    <img src="DiffFIT.jpg" alt="DiffFIT" style="width:80%;">    <figcaption>图1: DiffFIT原理</figcaption></figure><p>算法流程如下：</p><figure>    <img src="DiffFIT1.jpg" alt="DiffFIT" style="width:80%;">    <figcaption>图2: DiffFIT算法实现</figcaption></figure><p>论文并没有太多原理性论述，作者通过大量的实验来证明以DiffFIT方式微调可以在下游数据集上得到更好的表现。</p><h2 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h2><p>实验所选用的基础模型是DiT-XL&#x2F;2，加载在ImageNet256×256上的预训练7B大小的权重，使用学习率1e-4进行全量微调，评估使用的无分类器引导（cfg）为1.5，cfg为4.0用于可视化。在提出的DiffFIT上学习率放大10倍（1e-3）进行参数高效微调。</p><h3 id="应用于下游数据集"><a href="#应用于下游数据集" class="headerlink" title="应用于下游数据集"></a>应用于下游数据集</h3><p>在Food101, SUN397, DF-20M mini, Caltech101, CUB-200-2011, ArtBench-10, Oxford Flowers and Stanford Cars 8个数据集上微调，并对比全量微调和其他微调方法，通过比较FID来证明作者的微调方法更加高效。作者使用8张V100 GPUs，批大小设置为256，微调迭代240k次进行报告。</p><figure>    <img src="DiffFIT2.jpg" alt="DiffFIT" style="width:80%;">    <figcaption>图3: 下游数据集评测</figcaption></figure><p>尽管全量微调在3&#x2F;8的数据集上表现略好于DiffFIT，但是由于其必须更新所有参数，在时间成本上无法与DiffFIT相比。</p><h3 id="在ImageNet256×256预训练模型上微调-ImageNet-512-×-512数据集"><a href="#在ImageNet256×256预训练模型上微调-ImageNet-512-×-512数据集" class="headerlink" title="在ImageNet256×256预训练模型上微调 ImageNet 512 × 512数据集"></a>在ImageNet256×256预训练模型上微调 ImageNet 512 × 512数据集</h3><p>32个V100 GPUs，批大小1024并且迭代30k次，报告FID分数时采样步长为250。</p><figure>    <img src="DiffFIT3.jpg" alt="DiffFIT" style="width:80%;">    <figcaption>图4: ImageNet上的表现</figcaption></figure><p>从结果中可以看到DiffFIT微调得到了SOTA，FID值为3.02，且用更短的训练时间。</p><hr><p>之后作者在Food512上实验，加载在预训练IN256检查点上微调的Food256的检查点（猜测这个检查点是前面所介绍的下游数据集上的实验），并对比预训练IN512检查点，以及所设计的位置编码技巧。</p><figure>    <img src="DiffFIT5.jpg" alt="DiffFIT" style="width:80%;">    <figcaption>图5: 使用Food101对比所设计的位置编码</figcaption></figure><p>最终，作者发现利用预训练的 ImageNet 512×512 检查点所带来的 FID 性能与预训练并微调的 256×256 的 ImageNet 所实现的性能相似。此外，作者观察到，通过将所提出的位置编码技巧纳入微调过程，FID 性能略有改善。</p><h2 id="收敛性分析"><a href="#收敛性分析" class="headerlink" title="收敛性分析"></a>收敛性分析</h2><p>在上述实验基础上，为了观察模型收敛情况，作者每15k次迭代计算FID分数。</p><figure>    <img src="DiffFIT6.jpg" alt="DiffFIT" style="width:80%;"></figure><p>从图中可以看到，其他微调方法与DiffFIT最终都能有相似的FID，但是DiFFFIT训练更加高效，所需要更新的参数更少（0.12%），更快收敛。</p><p>消融实验中，探索哪些部分的改进对结果有更大的影响。我更关注学习率，作者认为参数的高效微调需要比预训练更大的学习率，因为预训练已经在一定程度上初始化了模型的大部分参数，更大的学习率可以帮助快速适应剩余参数到新的任务。而在实验中作者发现比预训练大10倍的学习率可以产生最佳结果。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>DiffFIT是一种简单高效的微调方法。还是那句话，简单优雅的东西往往能产生令人意外的效果，曾经word2vec论文被ICLR拒稿，时隔十年，被NeurlIPS评为实践检验奖；LORA作者也曾以太过简单为由被拒稿。</p><figure>    <img src="DiffFIT7.jpg" alt="DiffFIT" style="width:80%;"></figure><p>“Most of the things really work is often just simple but elegant.”</p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Diffusion Model</tag>
      
      <tag>Efficient Training</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文解读——DiT|Scalable Diffusion Models with Transformers</title>
    <link href="/blog/2023/12/27/2023/12/27/DiT/"/>
    <url>/blog/2023/12/27/2023/12/27/DiT/</url>
    
    <content type="html"><![CDATA[<h1 id="论文解读——DiT-Scalable-Diffusion-Models-with-Transformers"><a href="#论文解读——DiT-Scalable-Diffusion-Models-with-Transformers" class="headerlink" title="论文解读——DiT|Scalable Diffusion Models with Transformers"></a>论文解读——DiT|Scalable Diffusion Models with Transformers</h1><p><code>ICCV2023</code></p><blockquote><p>摘要：本文是由meta研究人员提出的基于transformer架构的扩散模型，将主干网络（backbone）UNet替换为应用潜空间patches的transformer结构（参考了ViTs网络结构）。主要贡献如下：</p><p>1.全新的扩散模型结构，主干网络替换为transformer结构，并将新的扩散模型称之为DiT。</p><p>2.主要探索：条件模块设计中设计了四种变体、patch size、transformer block architecture、model size。</p></blockquote><h2 id="Diffusion-Transformers"><a href="#Diffusion-Transformers" class="headerlink" title="Diffusion Transformers"></a>Diffusion Transformers</h2><p>DiT的整体结构沿用了潜在扩散模型（LDM），其中VAE、Scheduler等结构并未发生改变，且在训练DiT中加载预训练的模型，在反向去噪过程中，所使用的主干网络UNet被替换为Transformer结构，关于Transformer应用于图像处理请参考ViT模型，DiT主干网络参考了很多ViT的最佳实践。</p><h2 id="四种变体（four-variants-of-transformer-blocks-that-process-conditional-inputs-differently）"><a href="#四种变体（four-variants-of-transformer-blocks-that-process-conditional-inputs-differently）" class="headerlink" title="四种变体（four variants of transformer blocks that process conditional inputs differently）"></a>四种变体（four variants of transformer blocks that process conditional inputs differently）</h2><h3 id="In-context-conditioning"><a href="#In-context-conditioning" class="headerlink" title="In-context conditioning"></a>In-context conditioning</h3><p>这种方式是指将条件信息转化为token直接与输入序列拼接，然后在送入模型训练，这种方式无需修改ViT的结构，使用标准ViT块就可以实现，在最后一个Transformer块之后删除条件信息。</p><figure>    <img src="DiT.jpg" alt="DiT" style="width:80%;">    <figcaption>图1: ViT图像输入</figcaption></figure><p>类似于上图，演示了将图片变成序列并嵌入类别标记。</p><h3 id="Cross-attention-block"><a href="#Cross-attention-block" class="headerlink" title="Cross-attention block"></a>Cross-attention block</h3><p>使用交叉注意力，当使用交叉注意力时，模型不需要将类别标记与时间标记等条件信息嵌入序列，而是把它们作为单独的序列，通过交叉注意力将二者合并。这样的方式条件信息的种类可以更丰富，但是同时也会为模型增加最大的计算量。</p><h3 id="Adaptive-layer-norm-adaLN-block"><a href="#Adaptive-layer-norm-adaLN-block" class="headerlink" title="Adaptive layer norm (adaLN) block."></a>Adaptive layer norm (adaLN) block.</h3><p>主要是说标准层归一化被adaLN取代，通过时间 $t$ 与类别标记 $c$ 的嵌入向量回归得到缩放因子与偏移因子。</p><h3 id="adaLN-Zero-block"><a href="#adaLN-Zero-block" class="headerlink" title="adaLN-Zero block"></a>adaLN-Zero block</h3><p>在UNet中，每个块的最后一个卷积层都会进行0初始化，在实践中发现这样的初始化有利于加速模型收敛，所以在DiT的结构中，作者也对adaLN的参相关参数（缩放因子与偏移因子）进行了0初始化。</p><p>模型的详细配置参考下图，划分出四种不同的模型大小。</p><figure>    <img src="DiT1.jpg" alt="DiT" style="width:80%;">    <figcaption>图2: DiT不同模型尺寸</figcaption></figure><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><p>patches是指将图像按像素级分割，比如p&#x3D;2，图像尺寸为256×256，那么分割后的patches数量为(256&#x2F;&#x2F;2)×(256&#x2F;2)，DiT-XL&#x2F;2表示XLarge型号的模型，patch size是2×2。</p><figure>    <img src="DiT2.jpg" alt="DiT" style="width:50%;">    <figcaption>图3: DiT结构设计</figcaption></figure><p>在ImageNet 数据集 上以 256 × 256 和 512 × 512 图像分辨率上训练类条件潜在 DiT 模型，应用输出线性层0初始化，优化器使用adamW。学习率恒定，设置为1e-4，批量大小为256，唯一使用的数据增强是水平翻转。使用指数滑动平均（EMA）来处理模型权重，在评估阶段，全部使用EMA模型。此外，无其他系数的衰减。依然使用VAE将图像映射到隐空间，VAE加载现成的模型预训练参数，并在训练中保持不变。</p><h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>在评估时使用250个DDPM采样步骤报告FID-50K。</p><p>使用的是ADM的<a href="https://github.com/openai/guided-diffusion">tensorflow评估套件</a>。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><figure>    <img src="DiT3.jpg" alt="DiT" style="width:60%;">    <figcaption>图4: 4种条件策略对比</figcaption></figure><p>训练了四个最高Gflops的DiT-XL&#x2F;2 模型，每个模型都使用不同的块设计。四种条件策略下FID随训练步长的变化情况，可以看到应用adaLN-Zero的模型取得了最优的结果。对于后面所介绍的内容，所有模型都将使用 adaLN-Zero DiT 模块。</p><p>在model size（S、B、L、XL）以及patch size（8、4、2）的空间中缩放模型，作者观察到增加模型大小和减小patch大小可以显着改进扩散模型。</p><figure>    <img src="DiT4.jpg" alt="DiT" style="width:100%;">    <figcaption>图5</figcaption></figure><p>作者认为DiT的Gflops对于提高性能至关重要。图 6 的结果表明参数计数并不能唯一地确定 DiT 模型的质量。当模型大小保持不变并且补丁大小减小时，变压器的总参数实际上没有改变（实际上，总参数略有减少），并且仅 Gflops 增加。这些结果表明，缩放模型的Gflops 实际上是提高性能的关键。为了进一步研究这一点，作者在图 8 中针对模型 Gflops 绘制了 400K 训练步骤下的 FID-50K。结果表明，当总 Gflops 相似时，不同的 DiT 配置会获得相似的 FID 值（例如，DiT-S&#x2F;2 和 DiT- B&#x2F;4）。</p><figure>    <img src="DiT5.jpg" alt="DiT" style="width:60%;">    <figcaption>图6</figcaption></figure><figure>    <img src="DiT6.jpg" alt="DiT" style="width:60%;">    <figcaption>图7</figcaption></figure><p>作者在之后的实验中发现，更大的模型计算效率更高，相对于训练步数较少的大型 DiT 模型而言，小型 DiT 模型即使训练时间较长，最终也会变得计算效率低下。</p><p>之后作者使用最高Gflop的模型DiT-XL&#x2F;2，训练7M步，在ImageNet上达到了SOTA。</p><figure>    <img src="DiT7.jpg" alt="DiT" style="width:80%;">    <figcaption>图8</figcaption></figure><figure>    <img src="DiT8.jpg" alt="DiT" style="width:80%;">    <figcaption>图9</figcaption></figure><p>最后作者还测试了采样步长对于结果的影响。在400k训练步长上，使用[16,,32,64,128,256,1000]的采样步骤绘制FID曲线。</p><figure>    <img src="DiT9.jpg" alt="DiT" style="width:60%;">    <figcaption>图10</figcaption></figure><p>主要得到的结论是扩大采样步长并不能弥补模型计算量所带来的差距。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Diffusion Transformers (DiTs)，这是一种基于 Transformer 的简单扩散模型主干，其性能优于之前的 U-Net 模型，并继承了 Transformer 模型类出色的缩放特性。这将为后续的研究工作奠定基础，期待未来对于Transformer结构下扩散模型的探索。</p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Diffusion Model</tag>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>网络的渐进成长|Deep Growing Learning</title>
    <link href="/blog/2023/12/25/2023/12/25/dgl/"/>
    <url>/blog/2023/12/25/2023/12/25/dgl/</url>
    
    <content type="html"><![CDATA[<h1 id="Deep-Growing-Learning"><a href="#Deep-Growing-Learning" class="headerlink" title="Deep Growing Learning"></a>Deep Growing Learning</h1><p><code>ICCV2017</code></p><blockquote><p>摘要：本篇论文主要研究了半监督学习中模型训练时可能出现的过拟合问题，在深度神经网络上这样的问题尤为严重。文章主要贡献如下：</p><p>1.提出了一种全新的深度生长的学习方法，随着训练的进行，动态的生长新层来提高分类器的能力。</p><p>2.解决了半监督学习中模型过拟合问题，在MNIST和CIFAR-10数据集上表现比以往的方法更优秀。</p></blockquote><p>下面来主要介绍论文中提出的新颖的模型训练方法DGL。</p><h2 id="深度成长学习"><a href="#深度成长学习" class="headerlink" title="深度成长学习"></a>深度成长学习</h2><p>主要设计了四个模块，分别是生长子网络、固定子网络、监督子网络以及选择子网络。</p><figure>    <img src="DGL2.jpg" alt="DGL" style="width:90%;">    <figcaption>图1: DGL结构</figcaption></figure><h3 id="生长子网络"><a href="#生长子网络" class="headerlink" title="生长子网络"></a>生长子网络</h3><p>在生长子网络里提出了“组件”（building blocks），个人觉得类似于块，如果你了解ResNet的话就明白块是什么，文章针对于不同的数据集设计了不同的“组件”。在这里我举一个自己的例子，假设一个“组件”是由两层卷积、一层池化组成。在训练开始时，我们可能会使用较少的块训练，之后随着训练进行，对网络添加新的块，并将前一个块的参数复制到新加入的块中（可以对weights做一些调整），然后对新的网络继续训练，在未达到某一条件之前，网络会不断生长。</p><figure>    <img src="DGL1.jpg" alt="DGL" style="width:40%;">    <figcaption>图1: 增长方式</figcaption></figure><p>对于这篇文章来说，Upsample和残差连接这些都没有，</p><h3 id="固定子网络"><a href="#固定子网络" class="headerlink" title="固定子网络"></a>固定子网络</h3><p>固定子网络是指在经过卷积层之后的全连接层，作者固定了网络中最后的两个全连接层，每一个全连接层都加入dropout和非线性激活层。</p><h3 id="监督子网络"><a href="#监督子网络" class="headerlink" title="监督子网络"></a>监督子网络</h3><p>一个softmax层，该层所计算的结果会反向传播来更新梯度，是实际的模型输出层。</p><h3 id="选择子网络"><a href="#选择子网络" class="headerlink" title="选择子网络"></a>选择子网络</h3><p>一个softmax层，输出one-hot编码的向量，称之为logits。每个位置上的概率值，选出概率最高的作为预测类别。这个模块是为了用无标签数据集生成伪标签筛选出可信标签数据作为有监督学习的数据来训练。（对照模型结构图）</p><p>（半监督学习：首先使用少量标记数据训练模型，然后使用模型对未标记数据进行预测，将预测最自信的部分作为新的训练数据。）</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在半监督任务上，以往的训练方式模型在训练中容易过拟合，本篇论文主要贡献就是提出了深度成长网络，使用这种方式可以很好地解决之前的深度网络在半监督学习上训练过拟合问题。但这一工作为渐进学习做出了重要贡献，为模型训练方式提供了新的方案。往往高效的方式简单而优雅，看似普通的设计都是能够令人振奋的技术变革。</p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DeepLearning</tag>
      
      <tag>Progressive Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>无分类器指导|classifier-free guidance scale</title>
    <link href="/blog/2023/12/23/2023/12/23/cfg/"/>
    <url>/blog/2023/12/23/2023/12/23/cfg/</url>
    
    <content type="html"><![CDATA[<p><a href="https://getimg.ai/guides/interactive-guide-to-stable-diffusion-guidance-scale-parameter">原文链接</a></p><p>简单来说，CFG尺度（无分类器引导尺度）或引导尺度<span style="color: green;">(“CFG scale (classifier-free guidance scale) or guidance scale”)</span>是一个控制图像生成过程遵循文本提示程度的参数。 值越高，图像越贴近给定的文本输入。</p><p>但这并不意味着该值应始终设置为最大值，因为更多的指导意味着更少的多样性和质量。</p><p>当指导比例值设置为1时，文本提示将被忽略。严格遵循最大20，但图像质量较差。 最具“创意”和“艺术性”的结果通常在 7-12 的指导范围内生成。 但使用高达 15 的比例仍然会产生几乎没有artifacts的结果。</p><p>设置正确的值取决于您想要的结果以及文本提示的复杂性。 决定权在于您，但最好总是尝试不同的规模，看看更有创意的结果或严格遵守提示的结果是否更适合您的用例。</p><p>如果您尝试生成提示中指定的更微小细节的图像，则可以从 12 到 16 之间的更高指导比例开始。</p><p>我们希望我们的指南能帮助您了解稳定扩散中的 CFG 比例&#x2F;引导比例参数，并且您将利用所学知识来创造令人惊叹的东西。</p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Diffusion Model</tag>
      
      <tag>Text2Image</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文解读——Denoising Diffusion Probability Model原理与代码分析</title>
    <link href="/blog/2023/12/21/2023/12/21/DDPM/"/>
    <url>/blog/2023/12/21/2023/12/21/DDPM/</url>
    
    <content type="html"><![CDATA[<h1 id="论文解读——Denoising-Diffusion-Probability-Model原理与代码分析"><a href="#论文解读——Denoising-Diffusion-Probability-Model原理与代码分析" class="headerlink" title="论文解读——Denoising Diffusion Probability Model原理与代码分析"></a>论文解读——Denoising Diffusion Probability Model原理与代码分析</h1><p><code>neurips2020</code></p><blockquote><p>摘要：使用扩散模型生成高质量图像，扩散概率模型和朗之万动力学的去噪分数匹配之间的联系设计的加权变分界进行训练来获得最佳结果，在学术界被认为是最早的扩散模型，尽管2015年Jascha Sohl-Dickstein等人已经提出了扩散概率模型。</p><p>论文主要贡献：</p><p>1.作者证明了扩散模型能够生成高质量图像样本。</p><p>2.在假设马尔可夫过程的前提下，作者发现了扩散模型 与变分推理之间的联系，使得可以训练一个优秀的神经网络来生成高质量样本。</p></blockquote><h2 id="DDPM原理"><a href="#DDPM原理" class="headerlink" title="DDPM原理"></a>DDPM原理</h2><p>扩散模型（Diffusion Model）是一种基于物理热力学扩散思想的深度学习生成模型，包括前向扩散和反向扩散两个过程。不太懂的朋友，不妨想象一下，一滴墨水滴入盛满水的杯中逐渐扩散到整个系统的情景。非平衡热力学描述这滴墨水随时间推移的扩散过程中每一个“时间步”状态的概率分布——从一个初始的复杂概率分布逐步扩散变成简单均匀的分布。如果可以反向求出这个过程，那么就可以从简单分布中推导复杂分布。说起扩散模型，甚至可以追溯一下它的前辈——VAE（变分自编码器）和GAN（生成对抗网络），不过这不在本文的讨论范围。</p><p>公认最早的扩散模型DDPM（Denoising Diffusion Probabilistic Model），假设了扩散过程是一个马尔可夫过程（每一个时间步的状态仅有上一个时间步状态的概率分布加上当前时间步的高斯噪声得到），以及扩散过程的逆过程是高斯分布等。</p><p>DDPM的收敛速度与马尔可夫时间步长和样本空间大小（resolution和sample size）成正比。在DDPM中，逆向过程近似于前向过程中添加的高斯噪声；需要迭代所有数千个时间步才能生成一个样本批次。</p><figure>    <img src="DDPM.png" alt="DDPM" style="width:100%;">    <figcaption>图1: DDPM过程</figcaption></figure><p>如图1所示，$x_0$到$x_T$是对数据进行加噪的前向过程，DDPM假设了扩散过程是一个马尔可夫过程（每一个时间步的概率分布仅由上一个时间步状态的概率分布加上当前时间步的高斯噪声得到）。时间步$t-1$到时间步$t$的单步加噪过程的数学表达式如下：<br>$$<br>q(x_t|x_{t-1})&#x3D;\mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_tI)<br>$$</p><p>噪声的方差是一个位于区间$(0,1)$的固定值$\beta_t$确定，均值则由固定值$\beta_t$和当前时刻“带噪”的数据分布确定。</p><p>最终的噪声分布数学表达式如下：<br>$$<br>q(x_{1:T}|x_0)&#x3D;\prod_{t&#x3D;1}^{T}q(x_t|x_{t-1})<br>$$</p><p>设 $\alpha_t:&#x3D;1-\beta_t$，$\bar{\alpha}_{t}:&#x3D;\prod_{s&#x3D;1}^{t}\alpha_s$，则有：<br>$$<br>q(x_t|x_0)&#x3D;\mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I)<br>$$</p><p>$x_t$的公式则为：<br>$$<br>x_t(x_0,\epsilon)&#x3D;\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon<br>$$</p><p>前向过程是将数据噪声化的过程，前向过程的逆过程（反向过程）则是“去噪”的过程，即从随机噪声中迭代恢复出清晰数据的过程。DDPM将反向过程也视为一个马尔可夫链，这个马尔科夫链是由一系列神经网络参数化的高斯分布组成，也就是需要训练的扩散模型UNet。</p><p>从时间步$t$到时间步$t-1$的单步反向“去噪”过程的数学表达式如下：<br>$$<br>p(x_{t-1}|x_t,x_0)&#x3D;\mathcal{N}(x_{t-1};\tilde{\mu}(x_t,x_0),\tilde{\beta}_tI)<br>$$</p><p>UNet网络学习到的weights和bias就是反向过程的均值与方差（假设方差不固定需要学习，DDPM论文中固定了方差但在后续论文的改进中将其变为了可学习的参数），这涉及到了重参数化的思想，在这里简单介绍一下：</p><p>正如 $p(x_{t-1}|x_t,x_0)$所示 ，我们要知道反向过程估计的复杂高斯分布$\mathcal{N}(x;\mu,\sigma)$ 的均值与方差，并在该分布下进行采样，而这样的采样是无法被神经网络所学习的。于是我们从简单高斯分布 $\mathcal{N}(x;0,1)$采样 $z$ ，假设参数 $w$和 $b$ 使得 $\mu$ 与 $\sigma$可以通过UNet网络对$w$ 和 $b$的学习来得到，这样随机性就只存在于$\mathcal{N}(x;0,1)$中，即：<br>$$<br>\mathcal{N}(x;\mu,\sigma)&#x3D;w*\mathcal{N}(x;0,1)+b<br>$$</p><p>这就是<code>重参数化</code>，不过DDPM中固定了方差不变，只学习均值。通过重参数化，$\epsilon_t\sim\mathcal{N}(0, I)$可以得到采样：<br>$$<br>x_t &#x3D; \sqrt{\tilde{\alpha}} x_0+\sqrt{1-\tilde{\alpha}} \epsilon_t<br>$$</p><p>正是因为反向过程的每一步都是参数化的高斯分布，因此可以分别求高斯分布的均值和方差。通过推导，可以得到时间步$t-1$的高斯分布$q(x_{t-1}|x_t,x_0)$的均值与方差的数学表达式：<br>$$<br>\tilde{\beta}_t&#x3D;\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}·\beta_t<br>$$</p><p>$$<br>\tilde{\mu}(x_t,x_0)&#x3D;\frac{\sqrt\alpha_t(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t+\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}x_0<br>$$</p><p>则$x_{t-1}$的公式：<br>$$<br>x_{t-1}&#x3D;\frac{1}{\sqrt{\alpha}_t}(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon)<br>$$</p><p>扩散模型的优化目标数学表达式：<br>$$<br>L^{simple}_{t-1}&#x3D;E_{x_0,\epsilon\sim\mathcal{N}(0,I)}[||\epsilon-\epsilon_\theta(x_t,t)||^2]<br>$$</p><p>在DDIM论文中损失做了一些调整（如下）。现在的扩散模型几乎都采用了DDIM提出了新的采样方式（而非像DDPM一样的马尔可夫过程每一步的采样都依赖上一步的结果），这样的好处是大大提高了采样的效率，之后可能会写一篇关于DDIM的分析。<br>$$<br>\mathcal{L}_{final}&#x3D;\mathcal{L}_{mse}+\lambda\mathcal{L}_{vlb}<br>$$</p><h2 id="扩散过程"><a href="#扩散过程" class="headerlink" title="扩散过程"></a>扩散过程</h2><figure>    <img src="DDPM1.jpg" alt="DDPM" style="width:80%;">    <figcaption>图2: 扩散原理</figcaption></figure><p>一个简单的扩散模型工作的示意图，将输入数据$Z$不断添加高斯噪声，即Diffusion Process来生成带噪数据$Z_T$，之后通过U-Net网络。</p><p>使用U-Net网络你有两种选择，一种是直接预测最后的结果，一种是进行噪声的预测（Diffusers库中实现通过对噪声预测）。根据DIT论文中的表述，DDPM可能选择了对图像（Improvements in DDPMs over the past two years have largely been driven by improved sampling techniques, most notably classifierfree guidance, reformulating diffusion models to predict noise instead of pixels [DDPM] and using cascaded DDPM pipelines where low-resolution base diffusion models are trained in parallel with upsamplers）的预测，此后的研究改进并通过UNet预测噪声来优化网络参数。</p><figure>    <img src="DDPM2.jpg" alt="DDPM" style="width:120%;">    <figcaption>图3: 算法流程</figcaption></figure><p>Algorithm 1展示了完整的训练优化过程，损失经过简化变成一个简单的均方损失函数。Algorithm 2中第4行公式$\epsilon$代表预测噪声，反向过程（马尔科夫过程）需要逐步去噪，$\alpha_t$是时间步调度器（与时间步设置有关），其次是一个$\sigma_tz$，是因为DDPM中每次在去噪后的图像上会再加入一点噪声来增加生成的多样性。</p><figure>    <img src="DDPM3.png" alt="DDPM" style="width:70%;">    <figcaption>图4: 时间步长调度器超参变化曲线</figcaption></figure><p>简单实现扩散模型的训练过程。</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> UNet2DModel<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> DDPMScheduler<span class="hljs-keyword">import</span> torch<span class="hljs-comment"># create model</span>model = UNet2DModel(    sample_size=image_size,    in_channels=<span class="hljs-number">3</span>,    out_channels=<span class="hljs-number">3</span>,    layers_per_block=<span class="hljs-number">2</span>,    block_out_channels=(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">256</span>),    down_block_types=(        <span class="hljs-string">&quot;DownBlock2D&quot;</span>,        <span class="hljs-string">&quot;DownBlock2D&quot;</span>,        <span class="hljs-string">&quot;AttnDownBlock2D&quot;</span>,        <span class="hljs-string">&quot;AttnDownBlock2D&quot;</span>,    ),    up_block_types=(        <span class="hljs-string">&quot;AttnUpBlock2D&quot;</span>,        <span class="hljs-string">&quot;AttnUpBlock2D&quot;</span>,        <span class="hljs-string">&quot;UpBlock2D&quot;</span>,        <span class="hljs-string">&quot;UpBlock2D&quot;</span>,    ),)model.to(device)<span class="hljs-comment"># 设定噪声调度器</span>noise_scheduler = DDPMScheduler(    num_train_timesteps=<span class="hljs-number">1000</span>,    beta_schedule=<span class="hljs-string">&quot;squaredcos_cap_v2&quot;</span>,)<span class="hljs-comment"># 训练循环</span>optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="hljs-number">4e-4</span>)losses = []<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">30</span>):    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dataloader):        clean_images = batch[<span class="hljs-string">&quot;images&quot;</span>].to(device)        <span class="hljs-comment"># add noisy for image</span>        noise = torch.randn(clean_images.shape).to(clean_images.device)        bs = clean_images.shape[<span class="hljs-number">0</span>]        <span class="hljs-comment"># select a random timestep for every image</span>        timesteps = torch.randint(            <span class="hljs-number">0</span>, noise_scheduler.num_train_timesteps, (bs,),            device=clean_images.device        ).long()        <span class="hljs-comment"># Add noise to a clear image based on the noise amplitude at each timestep</span>        noisy_images = noise_scheduler.add_noise(            clean_images,            noise,            timesteps        )        <span class="hljs-comment"># obtain results</span>        noise_pred = model(noisy_images, timesteps, return_dict=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]        <span class="hljs-comment"># compute loss</span>        loss = F.mse_loss(noise_pred, noise)        <span class="hljs-comment"># backward</span>        loss.backward(loss)</code></pre></div><p>实验部分在CIFAR-10和CELEBA-HQ数据集上测试图像的生成能力，评估指标选用了FID与IS，更详细内容可以参考原<a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf">论文</a>。</p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Diffusion Model</tag>
      
      <tag>DeepLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文解读——Attention is all you need（理解transformer原理及实现）</title>
    <link href="/blog/2023/12/19/2023/12/19/transformer/"/>
    <url>/blog/2023/12/19/2023/12/19/transformer/</url>
    
    <content type="html"><![CDATA[<h1 id="论文解读——Attention-is-all-you-need（理解transformer原理及实现）"><a href="#论文解读——Attention-is-all-you-need（理解transformer原理及实现）" class="headerlink" title="论文解读——Attention is all you need（理解transformer原理及实现）"></a>论文解读——Attention is all you need（理解transformer原理及实现）</h1><p><code>neurips2017</code></p><p><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">论文地址</a></p><p><a href="https://github.com/Jiawei804/transformer">基于tensorflow2.x的实现</a></p><blockquote><p>摘要：本篇论文是由goole Brain在2017年发表的一篇论文，提出了一个广泛应用于自然语言处理任务的新模型，即Transformer。主要贡献 ：</p><p>1.Transformer是开创性的工作，引领了后来深度学习的发展，截止到2023年，除了在NLP领域，在其他模态下引用Transformer的思想和架构成为主流。</p><p>2.在NLP领域主导的架构是循环神经网络（RNN），Transformer摒弃了RNN的架构，实现了训练并行，提高了训练效率。</p><p>3.提出的自注意力机制（self-Attention）被广泛应用于其他网络，推动了深度学习的发展，是后来BERT、GPT等模型的基础。</p></blockquote><h2 id="Transformer究竟长什么样子？"><a href="#Transformer究竟长什么样子？" class="headerlink" title="Transformer究竟长什么样子？"></a>Transformer究竟长什么样子？</h2><p>话不多说，先上图。</p><figure>    <img src="1.jpg" alt="Transformer" style="width:100%;">    <figcaption style="font-size: 14px;">图1: Transofrmer结构</figcaption></figure><p>先从宏观上介绍，Transformer一共有两部分组成，第一部分是编码器Encoder（左），第二部分是解码器Decoder（右）。位置编码（Positional Encoding）来记录位置信息，Input Embedding、Output Embedding将输入向量转化为Embedding嵌入，然后将二者加起来喂入模型，Decoder最终的输出会通过全连接层和SoftMax得到词表的概率分布，最简单的方式是选出概率最高的词作为候选词。接下来来详细聊一聊内部的设计。</p><h2 id="编码器Encoder"><a href="#编码器Encoder" class="headerlink" title="编码器Encoder"></a>编码器Encoder</h2><p>我们从Inputs一步一步梳理。</p><p>假设我们的训练集只有一句文本，首先要通过分词器tokenizer转变成词id，tokenizer有三种划分方式：分别是char-level、word-level、sub-level。目前更流行的是词根分词（sub-level）。我们假设文本通过tokenizer得到一个$(1,40)$的一个向量，这个就是准备好的Inputs。</p><p>之后，我们将Inputs输入到Embedding层（假设Embedding维度为512），得到一个$(1,50,512)$的词嵌入向量（Embedding中会将Inputs变为one-hot编码之后做矩阵运算（1，50，vocab_size）@（vocab_size，512））。</p><figure>    <img src="2.jpg" alt="Transformer" style="width:100%;">    <figcaption style="font-size: 14px;">图2: Position Embedding</figcaption></figure><p>此论文中的提出的位置编码是确定的值，只与位置有关无需训练得到，pos就是词的位置（这里的词不是word而是sub），$d_{model}$是超参数，这里举例512，维度是$(50,512)$。之后我们将Position Embedding扩展一个维度与Embedding嵌入逐元素相加之后输入到Encoder。</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))</span><span class="hljs-comment"># PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</span><span class="hljs-comment"># pos 和 i</span><span class="hljs-comment"># pos.shape: (seq_len, 1)</span><span class="hljs-comment"># i.shape: (1, d_model)</span><span class="hljs-comment"># PE(pos, i).shape: (seq_len, d_model)</span><span class="hljs-comment"># 计算 pos / 10000^(2i/d_model)</span><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_angles</span>(<span class="hljs-params">pos, i, d_model</span>):    angles = <span class="hljs-number">1</span> / np.power(<span class="hljs-number">10000</span>, (<span class="hljs-number">2</span> * (i // <span class="hljs-number">2</span>)) / np.float32(d_model))    <span class="hljs-keyword">return</span> pos * angles<span class="hljs-comment"># 计算位置信息,sentence_length长度是多少，就有多少个位置要变为位置编码</span><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_position_embedding</span>(<span class="hljs-params">sentence_length, d_model</span>):            angle_rads = get_angles(np.arange(sentence_length)[:, np.newaxis],                            np.arange(d_model)[np.newaxis, :],                            d_model)    <span class="hljs-comment"># sines.shape: [seq_len, d_model / 2]</span>    <span class="hljs-comment"># cosines.shape: [seq_len, d_model / 2]</span>    sines = np.sin(angle_rads[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>])    cosines = np.cos(angle_rads[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>])    <span class="hljs-comment"># position_embedding.shape: [seq_len, d_model]</span>    position_embedding = np.concatenate([sines, cosines], axis=-<span class="hljs-number">1</span>)    <span class="hljs-comment"># 进行维度扩展，把[seq_len, d_model]，变为[1, seq_len, d_model]</span>    position_embedding = position_embedding[np.newaxis, ...]    <span class="hljs-comment"># 变为float32类型 </span>    <span class="hljs-keyword">return</span> tf.cast(position_embedding, dtype=tf.float32)</code></pre></div><figure>    <img src="3.jpg" alt="Transformer" style="width:80%;">    <figcaption style="font-size: 14px;">图3: Position Embedding热力图</figcaption></figure><p>接下来，我们拿一个Encoder举例，N×代表多个Encoder叠加。Encoder的第一个模块是多头注意力（Multi-Head Attention），也是Transformer的核心。</p><h3 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h3><p>多头注意力是由多个缩放点积注意力（自注意力Self-Attention）组成的。</p><figure>    <img src="4.jpg" alt="Transformer" style="width:80%;">    <figcaption style="font-size: 14px;">图4: 缩放点积注意力与注意力计算公式</figcaption></figure><p>多头注意力首先有三个全连接层$W_{Q}$，$W_{K}$，$W_{V}$，Encoder的输入经过这三个全连接层得到$Q、K、V$三个向量，维度都是$(1,50,512)$，之后拆分成8个缩放点积注意力，$q、k、v$维度变为$(1,8,50,64)$。然后计算Attention（公式如上$d_{k}$是$k$的最后一个维度，因为前面的层用的是$d_{model}$所以仍然是512）。每一个 Attention的维度$(1,1,50,64)$，之后再拼接起来就是$(1,8,50,64)$，之后再做维度变换变成$(1,50,512)$。之后做残差连接，然后层归一化（Add &amp; Norm）。</p><p>之后就就进入到一个前馈网络，前馈网络由两个全连接层组成，主要用来做放缩，之后同样做残差连接，然后层归一化（Add &amp; Norm）。最后得到Encoder的输出，也被称为Encoder Hidden State。</p><figure>    <img src="5.jpg" alt="Transformer" style="width:80%;">    <figcaption style="font-size: 14px;">图5: 缩放点积注意力与多头注意力</figcaption></figure><h2 id="解码器Decoder"><a href="#解码器Decoder" class="headerlink" title="解码器Decoder"></a>解码器Decoder</h2><p>解码器与编码器的层结构完全一样，只不过一个解码器有两个注意力层和一个前馈网络层。</p><p>第一个注意力是自己的注意力，第二个注意力是结合了Encoder Hidden State，即Encoder的输出作为</p><p>$K、V$，Decoder的第一个注意力层的输出作为$Q$，然后计算Attention分数。</p><p>最终Decoder模块的输出会经过全连接层和SoftMax来得到词表中每个词的概率值 ，选出概率最高的作为候选值。</p><h2 id="为什么可以实现并行训练？"><a href="#为什么可以实现并行训练？" class="headerlink" title="为什么可以实现并行训练？"></a>为什么可以实现并行训练？</h2><p>在实现细节里，还有很重要的一个部分就是掩码mask，Transformer通过掩码矩阵，使得模型在单步训练中只可以看到它应该看到的信息。在 mask 里，应该被忽略的我们会设成 1，应该被保留的会设成 0。如果 mask 相应位置上为 1，那么我们就给对应的 logits 加上一个超级小的负数， -1e-12， 这样，对应的 logits 也就变成了一个超级小的数。然后在计算 softmax 的时候，一个超级小的数的指数会无限接近与 0。也就是它对应的 attention 的权重就是 0 了。</p><figure>    <img src="6.jpg" alt="Transformer" style="width:80%;">    <figcaption style="font-size: 14px;">图5: mask掩码</figcaption></figure><p>对于Encoder，有两个mask，第一个mask是为了屏蔽掉padding部分，在最开始做数据处理时，需要对文本对齐，比如我们规定文本tokenizer之后的id长度最大为50。另外一个是类似的上三角矩阵。第一个词只能注意到自己，第二个词可以注意到第一个词。第三个词可以注意到前两个词，依次类推。。。</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 1. padding_mask</span><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_padding_mask</span>(<span class="hljs-params">batch_data</span>):    <span class="hljs-string">&quot;&quot;&quot;</span><span class="hljs-string">    :param batch_data: [batch_size, seq_len]</span><span class="hljs-string">    :return: [batch_size, 1, 1, seq_len]</span><span class="hljs-string">    &quot;&quot;&quot;</span>    padding_mask = tf.cast(tf.math.equal(batch_data, <span class="hljs-number">0</span>), dtype=tf.float32)    <span class="hljs-keyword">return</span> padding_mask[:, tf.newaxis, tf.newaxis, :]<span class="hljs-comment"># 2. look_ahead_mask(上三角矩阵，每个词只能注意到前面的没有预知未来的能力)</span><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_look_ahead_mask</span>(<span class="hljs-params">size</span>):    mask = <span class="hljs-number">1</span> - tf.linalg.band_part(tf.ones((size, size)), -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>)    <span class="hljs-keyword">return</span> mask <span class="hljs-comment"># (seq_len, seq_len)</span></code></pre></div><p>对于Decoder，先来说第二个注意力，在$Q*K^T$的运算后得logits，维度是$(…, seq_len_q, seq_len_k)$，logits需要加上Encoder的padding_mask，因为Decoder不需要花费精力去注意Encoder的padding部分。其次来看第一个注意力，第一个注意力与Encoder一样，训练过程中，每一个位置的词不应该看到后面位置的词，以及不应该花时间去注意padding的部分（对于训练而言）。</p><p>个人理解所谓的并行训练就是不需要等待输出结果，每一次Decoder的输入都会使用每个位置正确的词。在推理（预测）中，是不可以并行的，依然需要串行生成。</p><h2 id="学习率warm-up"><a href="#学习率warm-up" class="headerlink" title="学习率warm-up"></a>学习率warm-up</h2><p>这并不是开创性工作，但是这篇论文里用到了这个技术，那就来简单说一说，学习率预热是指在训练的初始阶段，学习率会从一个很小的值逐渐增加，根据设定的超参数决定在哪一步达到设置的值，之后保持不变，这种称之为warm-up_constant，还有很多变种其中包括先上升后下降。</p><figure>    <img src="7.jpg" alt="Transformer" style="width:60%;">    <figcaption style="font-size: 14px;">图5: warm-up学习率预热曲线</figcaption></figure><h2 id="机器翻译的评价指标——bleu"><a href="#机器翻译的评价指标——bleu" class="headerlink" title="机器翻译的评价指标——bleu"></a>机器翻译的评价指标——bleu</h2><p>论文里作者着眼NLP领域的机器翻译任务，采用bleu评价指标，在这里做一个介绍。</p><h3 id="什么是bleu？"><a href="#什么是bleu？" class="headerlink" title="什么是bleu？"></a>什么是bleu？</h3><p>BLEU（Bilingual Evaluation Understudy）是一种用于自动评估机器翻译质量的指标。它最初由Kishore Papineni等人在2002年提出，旨在解决人工评估翻译质量的主观性和费时性问题。BLEU的工作原理是将机器生成的翻译与人工参考翻译进行比较，并根据它们之间的相似性分配一个分数。我们可以通过微软的nltk框架来使用bleu指标。</p><p>BLEU的计算方式如下：</p><ol><li><p>对于翻译结果（机器生成的翻译），它会计算参考翻译（目标值）之间的 n-gram（连续的 n 个词或字符序列）匹配度。</p></li><li><p>对每个 n-gram 匹配，BLEU将其与候选翻译中的 n-gram 数量相比较，并采用一种修正的精确匹配度度量来计算得分。</p></li><li><p>然后，BLEU将各个 n-gram 匹配的得分合并，通过计算几何平均值来得出最终的 BLEU 分数。通常，BLEU 的分数在0到1之间，表示翻译的质量，越接近1表示越好。</p></li></ol><p><strong>公式如下：</strong><br>$$<br>bleu &#x3D; exp^{\sum weight*logP}*惩罚系数<br>$$</p><h3 id="什么是n-gram？"><a href="#什么是n-gram？" class="headerlink" title="什么是n-gram？"></a>什么是n-gram？</h3><div class="code-wrapper"><pre><code class="hljs">与其解释抽象的公式，不如来一段代码实战理解，请看下面这段代码。</code></pre></div><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.translate.bleu.score <span class="hljs-keyword">import</span> sentence_bleureference = [[<span class="hljs-string">&#x27;the&#x27;</span>,<span class="hljs-string">&#x27;quick&#x27;</span>,<span class="hljs-string">&#x27;brown&#x27;</span>,<span class="hljs-string">&#x27;fox&#x27;</span>,<span class="hljs-string">&#x27;jumped&#x27;</span>,<span class="hljs-string">&#x27;over&#x27;</span>,<span class="hljs-string">&#x27;the&#x27;</span>,<span class="hljs-string">&#x27;lazy&#x27;</span>,<span class="hljs-string">&#x27;dog&#x27;</span>]]candidate = [<span class="hljs-string">&#x27;the&#x27;</span>,<span class="hljs-string">&#x27;quick&#x27;</span>,<span class="hljs-string">&#x27;brown&#x27;</span>,<span class="hljs-string">&#x27;fox&#x27;</span>,<span class="hljs-string">&#x27;jumped&#x27;</span>,<span class="hljs-string">&#x27;over&#x27;</span>,<span class="hljs-string">&#x27;the&#x27;</span>,<span class="hljs-string">&#x27;lazy&#x27;</span>,<span class="hljs-string">&#x27;dog&#x27;</span>]score = sentence.bleu(reference, candidate)<span class="hljs-built_in">print</span>(score)<span class="hljs-comment"># out: 1.0</span><span class="hljs-comment"># 4-gram cumulative BLEU</span>candidate = [<span class="hljs-string">&#x27;the&#x27;</span>,<span class="hljs-string">&#x27;fast&#x27;</span>,<span class="hljs-string">&#x27;brown&#x27;</span>,<span class="hljs-string">&#x27;fox&#x27;</span>,<span class="hljs-string">&#x27;jumped&#x27;</span>,<span class="hljs-string">&#x27;over&#x27;</span>,<span class="hljs-string">&#x27;the&#x27;</span>,<span class="hljs-string">&#x27;lazy&#x27;</span>,<span class="hljs-string">&#x27;dog&#x27;</span>]score = sentence_bleu(reference, candidate, weights=(<span class="hljs-number">0.25</span>, <span class="hljs-number">0.25</span>, <span class="hljs-number">0.25</span>, <span class="hljs-number">0.25</span>))<span class="hljs-built_in">print</span>(score)<span class="hljs-comment"># out:0.75</span><span class="hljs-comment"># 手动计算</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npscore = np.exp(<span class="hljs-number">0.25</span> * (np.log(<span class="hljs-number">8</span>/<span class="hljs-number">9</span>) + np.log(<span class="hljs-number">6</span>/<span class="hljs-number">8</span>) + np.log(<span class="hljs-number">5</span>/<span class="hljs-number">7</span>) + np.log(<span class="hljs-number">4</span>/<span class="hljs-number">6</span>)))<span class="hljs-built_in">print</span>(score)<span class="hljs-comment"># out:0.75</span></code></pre></div><ul><li><p>过短惩罚系数，上例预测结果与目标结果长度一致，故为1。</p><ul><li>当预测结果比目标结果短时，过短惩罚系数就不为1了。</li></ul></li><li><p>在实际中，参考句可能不止一句，所以reference是二维。</p><ul><li>实际上，只要reference比candidate多一个维度就可以，candidate未必是一维的。</li></ul></li><li><p>n-gram实际上就是连续n个词一样。</p></li><li><p>weights是对每个gram赋予一个权重。</p></li><li><p>4-gram考查了1个，2个，3个，4个连续的词相同的情况。</p></li><li><p>P是连续n个词相同的概率。</p></li><li><p>bleu一定是一个0-1之间的值。</p><ul><li>P是0-1之间，logP是负数，乘以weights仍然是负数。</li><li>e的负数次幂在0-1之间。</li><li>过短惩罚系数是一个0-1之间的值。</li></ul></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以上是论文的主要贡献，开创性的Transformer架构成为了深度学习划时代的标志，像此后的GPT模型就是采用Decoder模块堆叠，他们的底层原理都来自于这一工作。谈到GPT3.5的成功，其实并不是谁都可以做的这么好，这是OpenAI在算法，工程和系统问题上的集大成之作。</p><p>在知乎等各种平台上，我们可以看到很多解读这篇论文的工作，作为一名在深度学习领域学习探索的学生，都应该认真学习这篇跨时代意义的论文。</p><p>“Don’t dwell on the past, the moments take  our breath away will actually measure the life.”</p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Transformer</tag>
      
      <tag>DeepLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高中|17级18班</title>
    <link href="/blog/2023/12/18/2023/12/18/high_school/"/>
    <url>/blog/2023/12/18/2023/12/18/high_school/</url>
    
    <content type="html"><![CDATA[<hr><div class="row justify-content-sm-center">    <div class="col mt-3 mt-md-0">        <img src="/blog/2023/12/18/2023/12/18/high_school/13.jpg" class="" title="高中去向">    </div></div><hr>]]></content>
    
    
    <categories>
      
      <category>沉思录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>thoughts</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + HEXO学习之旅</title>
    <link href="/blog/2023/12/13/2023/12/13/use_hexo/"/>
    <url>/blog/2023/12/13/2023/12/13/use_hexo/</url>
    
    <content type="html"><![CDATA[<div style="text-align: center;">     <h1>HEXO学习之旅</h1> </div><p>在最开始的时候，计划采用购买国内服务器来搭建站点，不过一直都没有实施，懒狗石锤，就是想的多做得少。不过后来在知乎上看到了一个回答，结识了一位同学，说是可以使用GitHub Pages+HEXO来搭建，于是我就开始了我的GitHub Pages + HEXO之旅。</p><h2 id="GitHub-Pages"><a href="#GitHub-Pages" class="headerlink" title="GitHub Pages"></a>GitHub Pages</h2><p>GitHub Pages是GitHub提供的一个静态网页托管服务，可以直接使用GitHub的仓库来托管网页，而且还可以绑定自己的域名，这样就可以使用自己的域名来访问网页了。</p><p>不过这一过程花费了我很长的时间，我第一次真正的下水实操配置域名，实现多域名解析，还有授权github actions等等，失败，失败，失败……，差一点放弃，不过最终还是成功了。这个过程很奇妙，我怀着热情，每天在晚上抽出几个小时，有时我会干到很晚，经常实验室就剩我一个人。</p><p>完成github pages以及github actions，主页就初步建成了。</p><img src="/blog/2023/12/13/2023/12/13/use_hexo/photo.jpg" class="" title="学院石碑"><h2 id="令人兴奋的开源项目"><a href="#令人兴奋的开源项目" class="headerlink" title="令人兴奋的开源项目"></a>令人兴奋的开源项目</h2><p>如果是我一个人手撕这个网站，那基本是猴年马月了，说真的，我对开源项目有崇高的敬意。还有那些热爱分享的年轻人，我的出发是一位网友引领的，我们从未相见过，只是因为在一篇帖子中请教问题，他回复了我。当我访问他的主页时，看到了他的博客，我之前一直有想法建一个属于自己的网页，于是乎我抱着尝试的心态询问了他。</p><p>我并没有想到他会回复我，他真的私信了我，令我感激的是，后来我遇到了一些麻烦，他依然愿意为我解答。这种感觉很棒，我想起linux课上，老师在黑板上写的一个单词”free“，他说，”这个单词不只是免费的意思，它代表着开源精神“。当然，开源并不等同于免费，我想我便爱上了开源。</p><h2 id="HEXO"><a href="#HEXO" class="headerlink" title="HEXO"></a>HEXO</h2><p>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他标记语言）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。这个框架我以前从未接触过，我打开官方网站，开始按着教程一步一步实践。我在本地搭建了一个Hexo，然后在本地写了一篇文章，部署到了GitHub Pages上，这个过程并不顺利，还好，现在你们看到了，它就在这里。”What you see is what you get“.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我很享受这样的过程，痛苦、兴奋、尝试，这些属于我自己的时间，总有一天会闪闪发光。当我现在回头再看，发现这些流程是如此简单，可能真的是这样的，当你身在局中的时候，反而看不到本质的东西。行文至此，该结束了，下面放一段最近很喜欢的话吧。</p><p>"Three passions, simple but overwhelmingly strong, have governed my life: the longing for love, the search for knowledge, and unbearable pity for the suffering of mankind. These passions, like great winds, have blown me hither and thither, in a wayward course, over a great ocean of anguish, reaching to the very verge of despair."</p> <p style="text-align:right">--罗素</p>]]></content>
    
    
    <categories>
      
      <category>沉思录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>thoughts</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
